<!DOCTYPE HTML>
<html>

<head>
  <title>User Manual</title>
  <meta name="description" content="website description" />
  <meta name="keywords" content="website keywords, website keywords" />
  <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
  <link rel="shortcut icon" type="image/png" href="static/image/favicon.ico" />
  <meta property="og:title" content="Inference Portal for multiple Large Language Models">
  <meta property="og:description" content="Inference Portal that offers API, Chat Bot Mode and more for multiple large language modelS">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://professorparakeet.com/static/image/android-chrome-192x192.png">
  <meta property="og:image" content="https://professorparakeet.com/static/image/android-chrome-192x192.png"/>
  <link rel="stylesheet" href="static/css/pico-main/css/pico.min.css" />
  <link rel="stylesheet" href="static/css/pico-main/css/pico.colors.min.css" />
</head>

<body>
  <header class="container">
    <nav>
      <ul>
        <hgroup  onclick="window.location.href = '/';">
          <h1><b>Inference</b></h1>
        </hgroup>
      </ul>
      <ul>
        <li>
          <details  class="dropdown">
            <summary role="button" class="outline">
              About
            </summary>
            <ul dir="rtl">
              <li><a  href="/">Introduction</a></li>
              <li><a  href="/model_infor">Model</a></li>
              <li><a aria-current="page" href="/manual">User Manual</a></li>
              <li><a href="/contact">Contact Us</a></li>
            </ul>
          </details> 
        </li>
       
        <li>
          <details  class="dropdown">
            <summary>
              Modes
            </summary>
            <ul dir="rtl">
              <li><a href="/prompt">Prompt and API</a></li>
              <li><a href="/chat">Chat Bot and Agent</a></li>
            </ul>
          </details>
        </li> 
        <li><a class="contrast" href="/buy">Get Key</a></li>
        <li><a class="contrast" href="/promptresponse">Response Log</a></li>
        
      </ul>
      
    </nav>
  
  </header>
  <hr>
  <main class="container">

      
      <div id="content" class="container">
        <!-- insert the page content here -->
        <h1>What can this website do?</h1>
        <p> This website is trying to sell you the access to several language models. The users can access the language
          models via three modes below: </p>
        <h1>API endpoints</h1>
        <p> This website fowards your request to the GPU servers and return the results. </p>
        <h4>Python example for chat endpoint:</h4>
        <code style="font-size: 12px">
r = requests.post("https://professorparakeet.com/api/chat", 
headers={"Authorization": "Bearer xxx"}, 
json={"prompt": str,
  "model" : str,
  'top_k': float,
  'top_p': float,
  'best_of': int,
  'max_tokens': int,
  'frequency_penalty': float,
  'presense_penalty': float,
  'temperature': float,
  'beam': bool,
  'early_stopping': bool,
  'length_penalty': float
}) 
<br>
print(r.json())
</code>
        <h4>cURL example for text completion endpoint:</h4>
        <code style="font-size: 12px">
curl "https://professorparakeet.com/api/completion" --header 
'Content-Type: application/json' 
--request POST
-H "Authorization: Bearer xxx" 
--data-binary
'{"prompt": str,
"model" : str,
"top_k": float,
"top_p": float,
"best_of": int,
"max_tokens": int,
"frequency_penalty": float,
"presense_penalty": float,
"temperature": float,
"beam': bool,
"early_stopping": bool,
"length_penalty": float
}'
</code>
        <small><i>NOTE: If the server is currently offline, you should send a warm up request to boot it up, otherwise you
          will get a lot a status reponses for your prompts.</i></small>
        <h1>Chat Bot Mode</h1>
        <p>We offer standard chatbot mode with custom chat template. The memory of the models are 10 previous
          promptresponse pairs associated with your Key or until the context reach 3000 tokens (so we have at least 1000
          tokens left to answer your lastest prompt).
        </p>
        <p>
          We are working on a vectorised database to better generate prompts from your chat history. We also offer an
          option to not storing your chatlog on our server.
          Therefore, please do not input a 10000-page book into the chatroom, the model will give you nothing. You can
          contribute or chew gum. </p>
        <h1>Hard Core Prompt Engineering Mode</h1>
        <p>We are working on a feature that use Lagent to split our model into multiple smart workers for complicated
          tasks that require multiple reasoning steps.
          User can participate into the creation of such prompts for their own tasks.
          You can contribute or chew gum. </p>
        <h1>The behaviors of this website</h1>
        <ul>
          <li>This website dynamically boot up GPU servers on users' demands</li>
          <li>1200 seconds from the last resonpses, GPU servers are automatically shut down. </li>
          <li>GPU servers are kept running if there is a constant demand. </li>
          <li>Global Rate Limit is 3 requests/second. API Rate Limit is 5 requests/second </li>
          <li>You can check and collect your promptresponse log in <a href="/promptresponse">Response Log</a>.
          </li>
        </ul>
        </ul>
      </div>
 

  </main>
</body>

</html>