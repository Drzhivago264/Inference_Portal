import { v4 as uuidv4 } from "uuid";
import { AIMessage, HumanMessage, HumanMessageChunk, AIMessageChunk, ToolMessageChunk, ChatMessageChunk, FunctionMessageChunk, isAIMessage, } from "@langchain/core/messages";
import { BaseChatModel, } from "@langchain/core/language_models/chat_models";
import { ChatGenerationChunk, } from "@langchain/core/outputs";
import { getEnvironmentVariable } from "@langchain/core/utils/env";
import { convertToOpenAITool } from "@langchain/core/utils/function_calling";
import { JsonOutputParser, StructuredOutputParser, } from "@langchain/core/output_parsers";
import { JsonOutputKeyToolsParser, convertLangChainToolCallToOpenAI, makeInvalidToolCall, parseToolCall, } from "@langchain/core/output_parsers/openai_tools";
import { RunnablePassthrough, RunnableSequence, } from "@langchain/core/runnables";
import { zodToJsonSchema } from "zod-to-json-schema";
function convertMessagesToMistralMessages(messages) {
    const getRole = (role) => {
        switch (role) {
            case "human":
                return "user";
            case "ai":
                return "assistant";
            case "system":
                return "system";
            case "tool":
                return "tool";
            case "function":
                return "assistant";
            default:
                throw new Error(`Unknown message type: ${role}`);
        }
    };
    const getContent = (content) => {
        if (typeof content === "string") {
            return content;
        }
        throw new Error(`ChatMistralAI does not support non text message content. Received: ${JSON.stringify(content, null, 2)}`);
    };
    const getTools = (message) => {
        if (isAIMessage(message) && !!message.tool_calls?.length) {
            return message.tool_calls
                .map((toolCall) => ({ ...toolCall, id: "null" }))
                .map(convertLangChainToolCallToOpenAI);
        }
        const toolCalls = message.additional_kwargs.tool_calls ?? [];
        return (toolCalls?.map((toolCall) => ({
            id: "null",
            type: "function",
            function: toolCall.function,
        })) || []);
    };
    return messages.map((message) => ({
        role: getRole(message._getType()),
        content: getContent(message.content),
        tool_calls: getTools(message),
    }));
}
function mistralAIResponseToChatMessage(choice) {
    const { message } = choice;
    // MistralAI SDK does not include tool_calls in the non
    // streaming return type, so we need to extract it like this
    // to satisfy typescript.
    let rawToolCalls = [];
    if ("tool_calls" in message && Array.isArray(message.tool_calls)) {
        rawToolCalls = message.tool_calls;
    }
    switch (message.role) {
        case "assistant": {
            const toolCalls = [];
            const invalidToolCalls = [];
            for (const rawToolCall of rawToolCalls) {
                try {
                    const parsed = parseToolCall(rawToolCall, { returnId: true });
                    toolCalls.push({
                        ...parsed,
                        id: parsed.id ?? uuidv4().replace(/-/g, ""),
                    });
                    // eslint-disable-next-line @typescript-eslint/no-explicit-any
                }
                catch (e) {
                    invalidToolCalls.push(makeInvalidToolCall(rawToolCall, e.message));
                }
            }
            return new AIMessage({
                content: message.content ?? "",
                tool_calls: toolCalls,
                invalid_tool_calls: invalidToolCalls,
                additional_kwargs: {
                    tool_calls: rawToolCalls,
                },
            });
        }
        default:
            return new HumanMessage(message.content ?? "");
    }
}
function _convertDeltaToMessageChunk(delta) {
    if (!delta.content && !delta.tool_calls) {
        return null;
    }
    // Our merge additional kwargs util function will throw unless there
    // is an index key in each tool object (as seen in OpenAI's) so we
    // need to insert it here.
    const rawToolCallChunksWithIndex = delta.tool_calls?.length
        ? delta.tool_calls?.map((toolCall, index) => ({
            ...toolCall,
            index,
            id: toolCall.id ?? uuidv4().replace(/-/g, ""),
            type: "function",
        }))
        : undefined;
    let role = "assistant";
    if (delta.role) {
        role = delta.role;
    }
    const content = delta.content ?? "";
    let additional_kwargs;
    const toolCallChunks = [];
    if (rawToolCallChunksWithIndex !== undefined) {
        additional_kwargs = {
            tool_calls: rawToolCallChunksWithIndex,
        };
        for (const rawToolCallChunk of rawToolCallChunksWithIndex) {
            toolCallChunks.push({
                name: rawToolCallChunk.function?.name,
                args: rawToolCallChunk.function?.arguments,
                id: rawToolCallChunk.id,
                index: rawToolCallChunk.index,
            });
        }
    }
    else {
        additional_kwargs = {};
    }
    if (role === "user") {
        return new HumanMessageChunk({ content });
    }
    else if (role === "assistant") {
        return new AIMessageChunk({
            content,
            tool_call_chunks: toolCallChunks,
            additional_kwargs,
        });
    }
    else if (role === "tool") {
        return new ToolMessageChunk({
            content,
            additional_kwargs,
            tool_call_id: rawToolCallChunksWithIndex?.[0].id ?? "",
        });
    }
    else if (role === "function") {
        return new FunctionMessageChunk({
            content,
            additional_kwargs,
        });
    }
    else {
        return new ChatMessageChunk({ content, role });
    }
}
function _convertStructuredToolToMistralTool(tools) {
    return tools.map((tool) => convertToOpenAITool(tool));
}
/**
 * Integration with a chat model.
 */
export class ChatMistralAI extends BaseChatModel {
    // Used for tracing, replace with the same name as your class
    static lc_name() {
        return "ChatMistralAI";
    }
    constructor(fields) {
        super(fields ?? {});
        Object.defineProperty(this, "modelName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "mistral-small-latest"
        });
        Object.defineProperty(this, "model", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "mistral-small-latest"
        });
        Object.defineProperty(this, "apiKey", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "endpoint", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "temperature", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 0.7
        });
        Object.defineProperty(this, "streaming", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "topP", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 1
        });
        Object.defineProperty(this, "maxTokens", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        /**
         * @deprecated use safePrompt instead
         */
        Object.defineProperty(this, "safeMode", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "safePrompt", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "randomSeed", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "seed", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "lc_serializable", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: true
        });
        const apiKey = fields?.apiKey ?? getEnvironmentVariable("MISTRAL_API_KEY");
        if (!apiKey) {
            throw new Error("API key MISTRAL_API_KEY is missing for MistralAI, but it is required.");
        }
        this.apiKey = apiKey;
        this.streaming = fields?.streaming ?? this.streaming;
        this.endpoint = fields?.endpoint;
        this.temperature = fields?.temperature ?? this.temperature;
        this.topP = fields?.topP ?? this.topP;
        this.maxTokens = fields?.maxTokens ?? this.maxTokens;
        this.safeMode = fields?.safeMode ?? this.safeMode;
        this.safePrompt = fields?.safePrompt ?? this.safePrompt;
        this.randomSeed = fields?.seed ?? fields?.randomSeed ?? this.seed;
        this.seed = this.randomSeed;
        this.modelName = fields?.model ?? fields?.modelName ?? this.model;
        this.model = this.modelName;
    }
    _llmType() {
        return "mistral_ai";
    }
    /**
     * Get the parameters used to invoke the model
     */
    invocationParams(options) {
        const { response_format, tools, tool_choice } = options ?? {};
        const mistralAITools = tools
            ?.map((tool) => {
            if ("lc_namespace" in tool) {
                return _convertStructuredToolToMistralTool([tool]);
            }
            return tool;
        })
            .flat();
        const params = {
            model: this.model,
            tools: mistralAITools,
            temperature: this.temperature,
            maxTokens: this.maxTokens,
            topP: this.topP,
            randomSeed: this.seed,
            safeMode: this.safeMode,
            safePrompt: this.safePrompt,
            toolChoice: tool_choice,
            responseFormat: response_format,
        };
        return params;
    }
    bindTools(tools, kwargs) {
        const mistralAITools = tools
            ?.map((tool) => {
            if ("lc_namespace" in tool) {
                return _convertStructuredToolToMistralTool([tool]);
            }
            return tool;
        })
            .flat();
        return this.bind({
            tools: mistralAITools,
            ...kwargs,
        });
    }
    async completionWithRetry(input, streaming) {
        const { MistralClient } = await this.imports();
        const client = new MistralClient(this.apiKey, this.endpoint);
        return this.caller.call(async () => {
            let res;
            if (streaming) {
                res = client.chatStream(input);
            }
            else {
                res = await client.chat(input);
            }
            return res;
        });
    }
    /** @ignore */
    async _generate(messages, options, runManager) {
        const tokenUsage = {};
        const params = this.invocationParams(options);
        const mistralMessages = convertMessagesToMistralMessages(messages);
        const input = {
            ...params,
            messages: mistralMessages,
        };
        // Enable streaming for signal controller or timeout due
        // to SDK limitations on canceling requests.
        const shouldStream = !!options.signal ?? !!options.timeout;
        // Handle streaming
        if (this.streaming || shouldStream) {
            const stream = this._streamResponseChunks(messages, options, runManager);
            const finalChunks = {};
            for await (const chunk of stream) {
                const index = chunk.generationInfo?.completion ?? 0;
                if (finalChunks[index] === undefined) {
                    finalChunks[index] = chunk;
                }
                else {
                    finalChunks[index] = finalChunks[index].concat(chunk);
                }
            }
            const generations = Object.entries(finalChunks)
                .sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10))
                .map(([_, value]) => value);
            return { generations, llmOutput: { estimatedTokenUsage: tokenUsage } };
        }
        // Not streaming, so we can just call the API once.
        const response = await this.completionWithRetry(input, false);
        const { completion_tokens: completionTokens, prompt_tokens: promptTokens, total_tokens: totalTokens, } = response?.usage ?? {};
        if (completionTokens) {
            tokenUsage.completionTokens =
                (tokenUsage.completionTokens ?? 0) + completionTokens;
        }
        if (promptTokens) {
            tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;
        }
        if (totalTokens) {
            tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;
        }
        const generations = [];
        for (const part of response?.choices ?? []) {
            if ("delta" in part) {
                throw new Error("Delta not supported in non-streaming mode.");
            }
            if (!("message" in part)) {
                throw new Error("No message found in the choice.");
            }
            const text = part.message?.content ?? "";
            const generation = {
                text,
                message: mistralAIResponseToChatMessage(part),
            };
            if (part.finish_reason) {
                generation.generationInfo = { finish_reason: part.finish_reason };
            }
            generations.push(generation);
        }
        return {
            generations,
            llmOutput: { tokenUsage },
        };
    }
    async *_streamResponseChunks(messages, options, runManager) {
        const mistralMessages = convertMessagesToMistralMessages(messages);
        const params = this.invocationParams(options);
        const input = {
            ...params,
            messages: mistralMessages,
        };
        const streamIterable = await this.completionWithRetry(input, true);
        for await (const data of streamIterable) {
            if (options.signal?.aborted) {
                throw new Error("AbortError");
            }
            const choice = data?.choices[0];
            if (!choice || !("delta" in choice)) {
                continue;
            }
            const { delta } = choice;
            if (!delta) {
                continue;
            }
            const newTokenIndices = {
                prompt: 0,
                completion: choice.index ?? 0,
            };
            const message = _convertDeltaToMessageChunk(delta);
            if (message === null) {
                // Do not yield a chunk if the message is empty
                continue;
            }
            const generationChunk = new ChatGenerationChunk({
                message,
                text: delta.content ?? "",
                generationInfo: newTokenIndices,
            });
            yield generationChunk;
            // eslint-disable-next-line no-void
            void runManager?.handleLLMNewToken(generationChunk.text ?? "", newTokenIndices, undefined, undefined, undefined, { chunk: generationChunk });
        }
    }
    /** @ignore */
    _combineLLMOutput() {
        return [];
    }
    withStructuredOutput(outputSchema, config) {
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        let schema;
        let name;
        let method;
        let includeRaw;
        if (isStructuredOutputMethodParams(outputSchema)) {
            schema = outputSchema.schema;
            name = outputSchema.name;
            method = outputSchema.method;
            includeRaw = outputSchema.includeRaw;
        }
        else {
            schema = outputSchema;
            name = config?.name;
            method = config?.method;
            includeRaw = config?.includeRaw;
        }
        let llm;
        let outputParser;
        if (method === "jsonMode") {
            llm = this.bind({
                response_format: { type: "json_object" },
            });
            if (isZodSchema(schema)) {
                outputParser = StructuredOutputParser.fromZodSchema(schema);
            }
            else {
                outputParser = new JsonOutputParser();
            }
        }
        else {
            let functionName = name ?? "extract";
            // Is function calling
            if (isZodSchema(schema)) {
                const asJsonSchema = zodToJsonSchema(schema);
                llm = this.bind({
                    tools: [
                        {
                            type: "function",
                            function: {
                                name: functionName,
                                description: asJsonSchema.description,
                                parameters: asJsonSchema,
                            },
                        },
                    ],
                    tool_choice: "auto",
                });
                outputParser = new JsonOutputKeyToolsParser({
                    returnSingle: true,
                    keyName: functionName,
                    zodSchema: schema,
                });
            }
            else {
                let openAIFunctionDefinition;
                if (typeof schema.name === "string" &&
                    typeof schema.parameters === "object" &&
                    schema.parameters != null) {
                    openAIFunctionDefinition = schema;
                    functionName = schema.name;
                }
                else {
                    openAIFunctionDefinition = {
                        name: functionName,
                        description: schema.description ?? "",
                        parameters: schema,
                    };
                }
                llm = this.bind({
                    tools: [
                        {
                            type: "function",
                            function: openAIFunctionDefinition,
                        },
                    ],
                    tool_choice: "auto",
                });
                outputParser = new JsonOutputKeyToolsParser({
                    returnSingle: true,
                    keyName: functionName,
                });
            }
        }
        if (!includeRaw) {
            return llm.pipe(outputParser);
        }
        const parserAssign = RunnablePassthrough.assign({
            // eslint-disable-next-line @typescript-eslint/no-explicit-any
            parsed: (input, config) => outputParser.invoke(input.raw, config),
        });
        const parserNone = RunnablePassthrough.assign({
            parsed: () => null,
        });
        const parsedWithFallback = parserAssign.withFallbacks({
            fallbacks: [parserNone],
        });
        return RunnableSequence.from([
            {
                raw: llm,
            },
            parsedWithFallback,
        ]);
    }
    /** @ignore */
    async imports() {
        const { default: MistralClient } = await import("@mistralai/mistralai");
        return { MistralClient };
    }
}
function isZodSchema(
// eslint-disable-next-line @typescript-eslint/no-explicit-any
input) {
    // Check for a characteristic method of Zod schemas
    return typeof input?.parse === "function";
}
function isStructuredOutputMethodParams(x
// eslint-disable-next-line @typescript-eslint/no-explicit-any
) {
    return (x !== undefined &&
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        typeof x.schema ===
            "object");
}
