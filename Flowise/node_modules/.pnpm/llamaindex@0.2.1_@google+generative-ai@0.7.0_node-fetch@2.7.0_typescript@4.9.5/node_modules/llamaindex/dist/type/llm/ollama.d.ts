import type { CallbackManager } from "../callbacks/CallbackManager.js";
import { BaseEmbedding } from "../embeddings/types.js";
import type { ChatMessage, ChatResponse, ChatResponseChunk, CompletionResponse, LLM, LLMChatParamsNonStreaming, LLMChatParamsStreaming, LLMCompletionParamsNonStreaming, LLMCompletionParamsStreaming, LLMMetadata } from "./types.js";
export declare class Ollama extends BaseEmbedding implements LLM {
    readonly hasStreaming = true;
    model: string;
    baseURL: string;
    temperature: number;
    topP: number;
    contextWindow: number;
    requestTimeout: number;
    additionalChatOptions?: Record<string, unknown>;
    callbackManager?: CallbackManager;
    protected modelMetadata: Partial<LLMMetadata>;
    constructor(init: Partial<Ollama> & {
        model: string;
        modelMetadata?: Partial<LLMMetadata>;
    });
    get metadata(): LLMMetadata;
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    private streamChat;
    complete(params: LLMCompletionParamsStreaming): Promise<AsyncIterable<CompletionResponse>>;
    complete(params: LLMCompletionParamsNonStreaming): Promise<CompletionResponse>;
    tokens(messages: ChatMessage[]): number;
    private getEmbedding;
    getTextEmbedding(text: string): Promise<number[]>;
    getQueryEmbedding(query: string): Promise<number[]>;
}
