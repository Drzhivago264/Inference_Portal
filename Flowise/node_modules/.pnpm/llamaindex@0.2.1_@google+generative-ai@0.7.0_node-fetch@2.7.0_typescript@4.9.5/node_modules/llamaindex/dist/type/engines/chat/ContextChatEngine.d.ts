import type { ChatHistory } from "../../ChatHistory.js";
import type { ContextSystemPrompt } from "../../Prompt.js";
import { Response } from "../../Response.js";
import type { BaseRetriever } from "../../Retriever.js";
import type { ChatMessage, LLM } from "../../llm/index.js";
import type { BaseNodePostprocessor } from "../../postprocessors/index.js";
import { PromptMixin } from "../../prompts/Mixin.js";
import type { ChatEngine, ChatEngineParamsNonStreaming, ChatEngineParamsStreaming, ContextGenerator } from "./types.js";
/**
 * ContextChatEngine uses the Index to get the appropriate context for each query.
 * The context is stored in the system prompt, and the chat history is preserved,
 * ideally allowing the appropriate context to be surfaced for each query.
 */
export declare class ContextChatEngine extends PromptMixin implements ChatEngine {
    chatModel: LLM;
    chatHistory: ChatHistory;
    contextGenerator: ContextGenerator;
    constructor(init: {
        retriever: BaseRetriever;
        chatModel?: LLM;
        chatHistory?: ChatMessage[];
        contextSystemPrompt?: ContextSystemPrompt;
        nodePostprocessors?: BaseNodePostprocessor[];
    });
    protected _getPromptModules(): Record<string, ContextGenerator>;
    chat(params: ChatEngineParamsStreaming): Promise<AsyncIterable<Response>>;
    chat(params: ChatEngineParamsNonStreaming): Promise<Response>;
    reset(): void;
    private prepareRequestMessages;
}
