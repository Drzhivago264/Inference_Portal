import type OpenAILLM from "openai";
import type { ClientOptions as OpenAIClientOptions } from "openai";
import type { CallbackManager, Event } from "../callbacks/CallbackManager.js";
import type { LLMOptions } from "portkey-ai";
import { Tokenizers } from "../GlobalsHelper.js";
import type { AnthropicSession } from "./anthropic.js";
import type { AzureOpenAIConfig } from "./azure.js";
import { BaseLLM } from "./base.js";
import type { OpenAISession } from "./open_ai.js";
import type { PortkeySession } from "./portkey.js";
import { ReplicateSession } from "./replicate_ai.js";
import type { ChatMessage, ChatResponse, ChatResponseChunk, LLMChatParamsNonStreaming, LLMChatParamsStreaming, LLMMetadata, MessageType } from "./types.js";
export declare const GPT4_MODELS: {
    "gpt-4": {
        contextWindow: number;
    };
    "gpt-4-32k": {
        contextWindow: number;
    };
    "gpt-4-32k-0613": {
        contextWindow: number;
    };
    "gpt-4-turbo-preview": {
        contextWindow: number;
    };
    "gpt-4-1106-preview": {
        contextWindow: number;
    };
    "gpt-4-0125-preview": {
        contextWindow: number;
    };
    "gpt-4-vision-preview": {
        contextWindow: number;
    };
};
export declare const GPT35_MODELS: {
    "gpt-3.5-turbo": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-0613": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-16k": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-16k-0613": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-1106": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-0125": {
        contextWindow: number;
    };
};
/**
 * We currently support GPT-3.5 and GPT-4 models
 */
export declare const ALL_AVAILABLE_OPENAI_MODELS: {
    "gpt-3.5-turbo": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-0613": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-16k": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-16k-0613": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-1106": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-0125": {
        contextWindow: number;
    };
    "gpt-4": {
        contextWindow: number;
    };
    "gpt-4-32k": {
        contextWindow: number;
    };
    "gpt-4-32k-0613": {
        contextWindow: number;
    };
    "gpt-4-turbo-preview": {
        contextWindow: number;
    };
    "gpt-4-1106-preview": {
        contextWindow: number;
    };
    "gpt-4-0125-preview": {
        contextWindow: number;
    };
    "gpt-4-vision-preview": {
        contextWindow: number;
    };
};
export declare const isFunctionCallingModel: (model: string) => boolean;
/**
 * OpenAI LLM implementation
 */
export declare class OpenAI extends BaseLLM {
    model: keyof typeof ALL_AVAILABLE_OPENAI_MODELS | string;
    temperature: number;
    topP: number;
    maxTokens?: number;
    additionalChatOptions?: Omit<Partial<OpenAILLM.Chat.ChatCompletionCreateParams>, "max_tokens" | "messages" | "model" | "temperature" | "top_p" | "stream" | "tools" | "toolChoice">;
    apiKey?: string;
    maxRetries: number;
    timeout?: number;
    session: OpenAISession;
    additionalSessionOptions?: Omit<Partial<OpenAIClientOptions>, "apiKey" | "maxRetries" | "timeout">;
    callbackManager?: CallbackManager;
    constructor(init?: Partial<OpenAI> & {
        azure?: AzureOpenAIConfig;
    });
    get metadata(): {
        model: string;
        temperature: number;
        topP: number;
        maxTokens: number | undefined;
        contextWindow: number;
        tokenizer: Tokenizers;
        isFunctionCallingModel: boolean;
    };
    tokens(messages: ChatMessage[]): number;
    mapMessageType(messageType: MessageType): "user" | "assistant" | "system" | "function" | "tool";
    toOpenAIMessage(messages: ChatMessage[]): {
        role: "function" | "user" | "assistant" | "system" | "tool";
        content: any;
    }[];
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    protected streamChat({ messages, parentEvent, }: LLMChatParamsStreaming): AsyncIterable<ChatResponseChunk>;
}
export declare const ALL_AVAILABLE_LLAMADEUCE_MODELS: {
    "Llama-2-70b-chat-old": {
        contextWindow: number;
        replicateApi: string;
    };
    "Llama-2-70b-chat-4bit": {
        contextWindow: number;
        replicateApi: string;
    };
    "Llama-2-13b-chat-old": {
        contextWindow: number;
        replicateApi: string;
    };
    "Llama-2-13b-chat-4bit": {
        contextWindow: number;
        replicateApi: string;
    };
    "Llama-2-7b-chat-old": {
        contextWindow: number;
        replicateApi: string;
    };
    "Llama-2-7b-chat-4bit": {
        contextWindow: number;
        replicateApi: string;
    };
};
export declare enum DeuceChatStrategy {
    A16Z = "a16z",
    META = "meta",
    METAWBOS = "metawbos",
    REPLICATE4BIT = "replicate4bit",
    REPLICATE4BITWNEWLINES = "replicate4bitwnewlines"
}
/**
 * Llama2 LLM implementation
 */
export declare class LlamaDeuce extends BaseLLM {
    model: keyof typeof ALL_AVAILABLE_LLAMADEUCE_MODELS;
    chatStrategy: DeuceChatStrategy;
    temperature: number;
    topP: number;
    maxTokens?: number;
    replicateSession: ReplicateSession;
    constructor(init?: Partial<LlamaDeuce>);
    tokens(messages: ChatMessage[]): number;
    get metadata(): {
        model: "Llama-2-70b-chat-old" | "Llama-2-70b-chat-4bit" | "Llama-2-13b-chat-old" | "Llama-2-13b-chat-4bit" | "Llama-2-7b-chat-old" | "Llama-2-7b-chat-4bit";
        temperature: number;
        topP: number;
        maxTokens: number | undefined;
        contextWindow: number;
        tokenizer: undefined;
    };
    mapMessagesToPrompt(messages: ChatMessage[]): {
        prompt: string;
        systemPrompt: any;
    };
    mapMessagesToPromptA16Z(messages: ChatMessage[]): {
        prompt: string;
        systemPrompt: undefined;
    };
    mapMessageTypeA16Z(messageType: MessageType): string;
    mapMessagesToPromptMeta(messages: ChatMessage[], opts?: {
        withBos?: boolean;
        replicate4Bit?: boolean;
        withNewlines?: boolean;
    }): {
        prompt: string;
        systemPrompt: any;
    };
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
}
export declare const ALL_AVAILABLE_ANTHROPIC_LEGACY_MODELS: {
    "claude-2.1": {
        contextWindow: number;
    };
    "claude-instant-1.2": {
        contextWindow: number;
    };
};
export declare const ALL_AVAILABLE_V3_MODELS: {
    "claude-3-opus": {
        contextWindow: number;
    };
    "claude-3-sonnet": {
        contextWindow: number;
    };
    "claude-3-haiku": {
        contextWindow: number;
    };
};
export declare const ALL_AVAILABLE_ANTHROPIC_MODELS: {
    "claude-3-opus": {
        contextWindow: number;
    };
    "claude-3-sonnet": {
        contextWindow: number;
    };
    "claude-3-haiku": {
        contextWindow: number;
    };
    "claude-2.1": {
        contextWindow: number;
    };
    "claude-instant-1.2": {
        contextWindow: number;
    };
};
/**
 * Anthropic LLM implementation
 */
export declare class Anthropic extends BaseLLM {
    model: keyof typeof ALL_AVAILABLE_ANTHROPIC_MODELS;
    temperature: number;
    topP: number;
    maxTokens?: number;
    apiKey?: string;
    maxRetries: number;
    timeout?: number;
    session: AnthropicSession;
    callbackManager?: CallbackManager;
    constructor(init?: Partial<Anthropic>);
    tokens(messages: ChatMessage[]): number;
    get metadata(): {
        model: "claude-3-opus" | "claude-3-sonnet" | "claude-3-haiku" | "claude-2.1" | "claude-instant-1.2";
        temperature: number;
        topP: number;
        maxTokens: number | undefined;
        contextWindow: number;
        tokenizer: undefined;
    };
    getModelName: (model: string) => string;
    formatMessages(messages: ChatMessage[]): {
        content: any;
        role: "user" | "assistant";
    }[];
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    protected streamChat(messages: ChatMessage[], parentEvent?: Event | undefined, systemPrompt?: string | null): AsyncIterable<ChatResponseChunk>;
}
export declare class Portkey extends BaseLLM {
    apiKey?: string;
    baseURL?: string;
    mode?: string;
    llms?: [LLMOptions] | null;
    session: PortkeySession;
    callbackManager?: CallbackManager;
    constructor(init?: Partial<Portkey>);
    tokens(messages: ChatMessage[]): number;
    get metadata(): LLMMetadata;
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    streamChat(messages: ChatMessage[], parentEvent?: Event, params?: Record<string, any>): AsyncIterable<ChatResponseChunk>;
}
