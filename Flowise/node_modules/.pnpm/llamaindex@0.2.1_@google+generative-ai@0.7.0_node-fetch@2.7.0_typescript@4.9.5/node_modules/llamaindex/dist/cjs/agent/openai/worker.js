"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
Object.defineProperty(exports, "OpenAIAgentWorker", {
    enumerable: true,
    get: function() {
        return OpenAIAgentWorker;
    }
});
const _env = require("@llamaindex/env");
const _Response = require("../../Response.js");
const _types = require("../../engines/chat/types.js");
const _index = require("../../llm/index.js");
const _utils = require("../../llm/utils.js");
const _ChatMemoryBuffer = require("../../memory/ChatMemoryBuffer.js");
const _utils1 = require("../../tools/utils.js");
const _types1 = require("../types.js");
const _utils2 = require("../utils.js");
const _utils3 = require("./utils.js");
const DEFAULT_MAX_FUNCTION_CALLS = 5;
/**
 * Call function.
 * @param tools: tools
 * @param toolCall: tool call
 * @param verbose: verbose
 * @returns: void
 */ async function callFunction(tools, toolCall, verbose = false) {
    const id_ = toolCall.id;
    const functionCall = toolCall.function;
    const name = toolCall.function.name;
    const argumentsStr = toolCall.function.arguments;
    if (verbose) {
        console.log("=== Calling Function ===");
        console.log(`Calling function: ${name} with args: ${argumentsStr}`);
    }
    const tool = (0, _utils2.getFunctionByName)(tools, name);
    const argumentDict = JSON.parse(argumentsStr);
    // Call tool
    // Use default error message
    const output = await (0, _utils1.callToolWithErrorHandling)(tool, argumentDict, null);
    if (verbose) {
        console.log(`Got output ${output}`);
        console.log("==========================");
    }
    return [
        {
            content: String(output),
            role: "tool",
            additionalKwargs: {
                name,
                tool_call_id: id_
            }
        },
        output
    ];
}
class OpenAIAgentWorker {
    llm;
    verbose;
    maxFunctionCalls;
    prefixMessages;
    callbackManager;
    _getTools;
    /**
   * Initialize.
   */ constructor({ tools = [], llm, prefixMessages, verbose, maxFunctionCalls = DEFAULT_MAX_FUNCTION_CALLS, callbackManager, toolRetriever }){
        this.llm = llm ?? new _index.OpenAI({
            model: "gpt-3.5-turbo-0613"
        });
        this.verbose = verbose || false;
        this.maxFunctionCalls = maxFunctionCalls;
        this.prefixMessages = prefixMessages || [];
        this.callbackManager = callbackManager || this.llm.callbackManager;
        if (tools.length > 0 && toolRetriever) {
            throw new Error("Cannot specify both tools and tool_retriever");
        } else if (tools.length > 0) {
            this._getTools = async ()=>tools;
        } else if (toolRetriever) {
            this._getTools = async (message)=>toolRetriever.retrieve(message);
        } else {
            this._getTools = async ()=>[];
        }
    }
    /**
   * Get all messages.
   * @param task: task
   * @returns: messages
   */ getAllMessages(task) {
        return [
            ...this.prefixMessages,
            ...task.memory.get(),
            ...task.extraState.newMemory.get()
        ];
    }
    /**
   * Get latest tool calls.
   * @param task: task
   * @returns: tool calls
   */ getLatestToolCalls(task) {
        const chatHistory = task.extraState.newMemory.getAll();
        if (chatHistory.length === 0) {
            return null;
        }
        return chatHistory[chatHistory.length - 1].additionalKwargs?.toolCalls;
    }
    /**
   *
   * @param task
   * @param openaiTools
   * @param toolChoice
   * @returns
   */ _getLlmChatKwargs(task, openaiTools, toolChoice = "auto") {
        const llmChatKwargs = {
            messages: this.getAllMessages(task)
        };
        if (openaiTools.length > 0) {
            llmChatKwargs.tools = openaiTools;
            llmChatKwargs.toolChoice = toolChoice;
        }
        return llmChatKwargs;
    }
    /**
   * Process message.
   * @param task: task
   * @param chatResponse: chat response
   * @returns: agent chat response
   */ _processMessage(task, chatResponse) {
        const aiMessage = chatResponse.message;
        task.extraState.newMemory.put(aiMessage);
        return new _types.AgentChatResponse(aiMessage.content, task.extraState.sources);
    }
    async _getStreamAiResponse(task, llmChatKwargs) {
        const stream = await this.llm.chat({
            stream: true,
            ...llmChatKwargs
        });
        const iterator = (0, _utils.streamConverter)((0, _utils.streamReducer)({
            stream,
            initialValue: "",
            reducer: (accumulator, part)=>accumulator += part.delta,
            finished: (accumulator)=>{
                task.extraState.newMemory.put({
                    content: accumulator,
                    role: "assistant"
                });
            }
        }), (r)=>new _Response.Response(r.delta));
        return new _types.StreamingAgentChatResponse(iterator, task.extraState.sources);
    }
    /**
   * Get agent response.
   * @param task: task
   * @param mode: mode
   * @param llmChatKwargs: llm chat kwargs
   * @returns: agent chat response
   */ async _getAgentResponse(task, mode, llmChatKwargs) {
        if (mode === _types.ChatResponseMode.WAIT) {
            const chatResponse = await this.llm.chat({
                stream: false,
                ...llmChatKwargs
            });
            return this._processMessage(task, chatResponse);
        } else if (mode === _types.ChatResponseMode.STREAM) {
            return this._getStreamAiResponse(task, llmChatKwargs);
        }
        throw new Error("Invalid mode");
    }
    /**
   * Call function.
   * @param tools: tools
   * @param toolCall: tool call
   * @param memory: memory
   * @param sources: sources
   * @returns: void
   */ async callFunction(tools, toolCall) {
        const functionCall = toolCall.function;
        if (!functionCall) {
            throw new Error("Invalid tool_call object");
        }
        const functionMessage = await callFunction(tools, toolCall, this.verbose);
        const message = functionMessage[0];
        const toolOutput = functionMessage[1];
        return {
            message,
            toolOutput
        };
    }
    /**
   * Initialize step.
   * @param task: task
   * @param kwargs: kwargs
   * @returns: task step
   */ initializeStep(task, kwargs) {
        const sources = [];
        const newMemory = new _ChatMemoryBuffer.ChatMemoryBuffer();
        const taskState = {
            sources,
            nFunctionCalls: 0,
            newMemory
        };
        task.extraState = {
            ...task.extraState,
            ...taskState
        };
        return new _types1.TaskStep(task.taskId, (0, _env.randomUUID)(), task.input);
    }
    /**
   * Should continue.
   * @param toolCalls: tool calls
   * @param nFunctionCalls: number of function calls
   * @returns: boolean
   */ _shouldContinue(toolCalls, nFunctionCalls) {
        if (nFunctionCalls > this.maxFunctionCalls) {
            return false;
        }
        if (toolCalls?.length === 0) {
            return false;
        }
        return true;
    }
    /**
   * Get tools.
   * @param input: input
   * @returns: tools
   */ async getTools(input) {
        return this._getTools(input);
    }
    async _runStep(step, task, mode = _types.ChatResponseMode.WAIT, toolChoice = "auto") {
        const tools = await this.getTools(task.input);
        if (step.input) {
            (0, _utils2.addUserStepToMemory)(step, task.extraState.newMemory, this.verbose);
        }
        const openaiTools = tools.map((tool)=>(0, _utils3.toOpenAiTool)({
                name: tool.metadata.name,
                description: tool.metadata.description,
                parameters: tool.metadata.parameters
            }));
        const llmChatKwargs = this._getLlmChatKwargs(task, openaiTools, toolChoice);
        const agentChatResponse = await this._getAgentResponse(task, mode, llmChatKwargs);
        const latestToolCalls = this.getLatestToolCalls(task) || [];
        let isDone;
        let newSteps = [];
        if (!this._shouldContinue(latestToolCalls, task.extraState.nFunctionCalls)) {
            isDone = true;
            newSteps = [];
        } else {
            isDone = false;
            for (const toolCall of latestToolCalls){
                const { message, toolOutput } = await this.callFunction(tools, toolCall);
                task.extraState.sources.push(toolOutput);
                task.extraState.newMemory.put(message);
                task.extraState.nFunctionCalls += 1;
            }
            newSteps = [
                step.getNextStep((0, _env.randomUUID)(), undefined)
            ];
        }
        return new _types1.TaskStepOutput(agentChatResponse, step, newSteps, isDone);
    }
    /**
   * Run step.
   * @param step: step
   * @param task: task
   * @param kwargs: kwargs
   * @returns: task step output
   */ async runStep(step, task, kwargs) {
        const toolChoice = kwargs?.toolChoice || "auto";
        return this._runStep(step, task, _types.ChatResponseMode.WAIT, toolChoice);
    }
    /**
   * Stream step.
   * @param step: step
   * @param task: task
   * @param kwargs: kwargs
   * @returns: task step output
   */ async streamStep(step, task, kwargs) {
        const toolChoice = kwargs?.toolChoice || "auto";
        return this._runStep(step, task, _types.ChatResponseMode.STREAM, toolChoice);
    }
    /**
   * Finalize task.
   * @param task: task
   * @param kwargs: kwargs
   * @returns: void
   */ finalizeTask(task, kwargs) {
        task.memory.set(task.memory.get().concat(task.extraState.newMemory.get()));
        task.extraState.newMemory.reset();
    }
}
