"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
Object.defineProperty(exports, "ReActAgentWorker", {
    enumerable: true,
    get: function() {
        return ReActAgentWorker;
    }
});
const _env = require("@llamaindex/env");
const _CallbackManager = require("../../callbacks/CallbackManager.js");
const _index = require("../../engines/chat/index.js");
const _index1 = require("../../llm/index.js");
const _ChatMemoryBuffer = require("../../memory/ChatMemoryBuffer.js");
const _index2 = require("../../tools/index.js");
const _types = require("../types.js");
const _formatter = require("./formatter.js");
const _outputParser = require("./outputParser.js");
const _types1 = require("./types.js");
/**
 *
 * @param step
 * @param memory
 * @param currentReasoning
 * @param verbose
 */ function addUserStepToReasoning(step, memory, currentReasoning, verbose = false) {
    if (step.stepState.isFirst) {
        memory.put({
            content: step.input,
            role: "user"
        });
        step.stepState.isFirst = false;
    } else {
        const reasoningStep = new _types1.ObservationReasoningStep({
            observation: step.input ?? undefined
        });
        currentReasoning.push(reasoningStep);
        if (verbose) {
            console.log(`Added user message to memory: ${step.input}`);
        }
    }
}
class ReActAgentWorker {
    llm;
    verbose;
    maxInteractions = 10;
    reactChatFormatter;
    outputParser;
    callbackManager;
    _getTools;
    constructor({ tools, llm, maxInteractions, reactChatFormatter, outputParser, callbackManager, verbose, toolRetriever }){
        this.llm = llm ?? new _index1.OpenAI({
            model: "gpt-3.5-turbo-0613"
        });
        this.callbackManager = callbackManager || new _CallbackManager.CallbackManager();
        this.maxInteractions = maxInteractions ?? 10;
        this.reactChatFormatter = reactChatFormatter ?? new _formatter.ReActChatFormatter();
        this.outputParser = outputParser ?? new _outputParser.ReActOutputParser();
        this.verbose = verbose || false;
        if (tools.length > 0 && toolRetriever) {
            throw new Error("Cannot specify both tools and tool_retriever");
        } else if (tools.length > 0) {
            this._getTools = async ()=>tools;
        } else if (toolRetriever) {
            this._getTools = async (message)=>toolRetriever.retrieve(message);
        } else {
            this._getTools = async ()=>[];
        }
    }
    /**
   * Initialize a task step.
   * @param task - task
   * @param kwargs - keyword arguments
   * @returns - task step
   */ initializeStep(task, kwargs) {
        const sources = [];
        const currentReasoning = [];
        const newMemory = new _ChatMemoryBuffer.ChatMemoryBuffer();
        const taskState = {
            sources,
            currentReasoning,
            newMemory
        };
        task.extraState = {
            ...task.extraState,
            ...taskState
        };
        return new _types.TaskStep(task.taskId, (0, _env.randomUUID)(), task.input, {
            isFirst: true
        });
    }
    /**
   * Extract reasoning step from chat response.
   * @param output - chat response
   * @param isStreaming - whether the chat response is streaming
   * @returns - [message content, reasoning steps, is done]
   */ extractReasoningStep(output, isStreaming) {
        if (!output.message.content) {
            throw new Error("Got empty message.");
        }
        const messageContent = output.message.content;
        const currentReasoning = [];
        let reasoningStep;
        try {
            reasoningStep = this.outputParser.parse(messageContent, isStreaming);
        } catch (e) {
            throw new Error(`Could not parse output: ${e}`);
        }
        if (this.verbose) {
            console.log(`${reasoningStep.getContent()}\n`);
        }
        currentReasoning.push(reasoningStep);
        if (reasoningStep.isDone()) {
            return [
                messageContent,
                currentReasoning,
                true
            ];
        }
        const actionReasoningStep = new _types1.ActionReasoningStep({
            thought: reasoningStep.getContent(),
            action: reasoningStep.action,
            actionInput: reasoningStep.actionInput
        });
        if (!(actionReasoningStep instanceof _types1.ActionReasoningStep)) {
            throw new Error(`Expected ActionReasoningStep, got ${reasoningStep}`);
        }
        return [
            messageContent,
            currentReasoning,
            false
        ];
    }
    /**
   * Process actions.
   * @param task - task
   * @param tools - tools
   * @param output - chat response
   * @param isStreaming - whether the chat response is streaming
   * @returns - [reasoning steps, is done]
   */ async _processActions(task, tools, output, isStreaming = false) {
        const toolsDict = {};
        for (const tool of tools){
            toolsDict[tool.metadata.name] = tool;
        }
        const [_, currentReasoning, isDone] = this.extractReasoningStep(output, isStreaming);
        if (isDone) {
            return [
                currentReasoning,
                true
            ];
        }
        const reasoningStep = currentReasoning[currentReasoning.length - 1];
        const actionReasoningStep = new _types1.ActionReasoningStep({
            thought: reasoningStep.getContent(),
            action: reasoningStep.action,
            actionInput: reasoningStep.actionInput
        });
        const tool = toolsDict[actionReasoningStep.action];
        const toolOutput = await tool?.call?.(actionReasoningStep.actionInput);
        task.extraState.sources.push(new _index2.ToolOutput(toolOutput, tool.metadata.name, actionReasoningStep.actionInput, toolOutput));
        const observationStep = new _types1.ObservationReasoningStep({
            observation: toolOutput
        });
        currentReasoning.push(observationStep);
        if (this.verbose) {
            console.log(`${observationStep.getContent()}`);
        }
        return [
            currentReasoning,
            false
        ];
    }
    /**
   * Get response.
   * @param currentReasoning - current reasoning steps
   * @param sources - tool outputs
   * @returns - agent chat response
   */ _getResponse(currentReasoning, sources) {
        if (currentReasoning.length === 0) {
            throw new Error("No reasoning steps were taken.");
        } else if (currentReasoning.length === this.maxInteractions) {
            throw new Error("Reached max iterations.");
        }
        const responseStep = currentReasoning[currentReasoning.length - 1];
        let responseStr;
        if (responseStep instanceof _types1.ResponseReasoningStep) {
            responseStr = responseStep.response;
        } else {
            responseStr = responseStep.getContent();
        }
        return new _index.AgentChatResponse(responseStr, sources);
    }
    /**
   * Get task step response.
   * @param agentResponse - agent chat response
   * @param step - task step
   * @param isDone - whether the task is done
   * @returns - task step output
   */ _getTaskStepResponse(agentResponse, step, isDone) {
        let newSteps = [];
        if (isDone) {
            newSteps = [];
        } else {
            newSteps = [
                step.getNextStep((0, _env.randomUUID)(), undefined)
            ];
        }
        return new _types.TaskStepOutput(agentResponse, step, newSteps, isDone);
    }
    /**
   * Run a task step.
   * @param step - task step
   * @param task - task
   * @param kwargs - keyword arguments
   * @returns - task step output
   */ async _runStep(step, task, kwargs) {
        if (step.input) {
            addUserStepToReasoning(step, task.extraState.newMemory, task.extraState.currentReasoning, this.verbose);
        }
        const tools = await this._getTools(task.input);
        const inputChat = this.reactChatFormatter.format(tools, [
            ...task.memory.getAll(),
            ...task.extraState.newMemory.getAll()
        ], task.extraState.currentReasoning);
        const chatResponse = await this.llm.chat({
            messages: inputChat
        });
        const [reasoningSteps, isDone] = await this._processActions(task, tools, chatResponse);
        task.extraState.currentReasoning.push(...reasoningSteps);
        const agentResponse = this._getResponse(task.extraState.currentReasoning, task.extraState.sources);
        if (isDone) {
            task.extraState.newMemory.put({
                content: agentResponse.response,
                role: "assistant"
            });
        }
        return this._getTaskStepResponse(agentResponse, step, isDone);
    }
    /**
   * Run a task step.
   * @param step - task step
   * @param task - task
   * @param kwargs - keyword arguments
   * @returns - task step output
   */ async runStep(step, task, kwargs) {
        return await this._runStep(step, task);
    }
    /**
   * Run a task step.
   * @param step - task step
   * @param task - task
   * @param kwargs - keyword arguments
   * @returns - task step output
   */ streamStep(step, task, kwargs) {
        throw new Error("Method not implemented.");
    }
    /**
   * Finalize a task.
   * @param task - task
   * @param kwargs - keyword arguments
   */ finalizeTask(task, kwargs) {
        task.memory.set(task.memory.get() + task.extraState.newMemory.get());
        task.extraState.newMemory.reset();
    }
}
