"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
Object.defineProperty(exports, "ContextChatEngine", {
    enumerable: true,
    get: function() {
        return ContextChatEngine;
    }
});
const _env = require("@llamaindex/env");
const _ChatHistory = require("../../ChatHistory.js");
const _Response = require("../../Response.js");
const _index = require("../../llm/index.js");
const _utils = require("../../llm/utils.js");
const _Mixin = require("../../prompts/Mixin.js");
const _DefaultContextGenerator = require("./DefaultContextGenerator.js");
class ContextChatEngine extends _Mixin.PromptMixin {
    chatModel;
    chatHistory;
    contextGenerator;
    constructor(init){
        super();
        this.chatModel = init.chatModel ?? new _index.OpenAI({
            model: "gpt-3.5-turbo-16k"
        });
        this.chatHistory = (0, _ChatHistory.getHistory)(init?.chatHistory);
        this.contextGenerator = new _DefaultContextGenerator.DefaultContextGenerator({
            retriever: init.retriever,
            contextSystemPrompt: init?.contextSystemPrompt,
            nodePostprocessors: init?.nodePostprocessors
        });
    }
    _getPromptModules() {
        return {
            contextGenerator: this.contextGenerator
        };
    }
    async chat(params) {
        const { message, stream } = params;
        const chatHistory = params.chatHistory ? (0, _ChatHistory.getHistory)(params.chatHistory) : this.chatHistory;
        const parentEvent = {
            id: (0, _env.randomUUID)(),
            type: "wrapper",
            tags: [
                "final"
            ]
        };
        const requestMessages = await this.prepareRequestMessages(message, chatHistory, parentEvent);
        if (stream) {
            const stream = await this.chatModel.chat({
                messages: requestMessages.messages,
                parentEvent,
                stream: true
            });
            return (0, _utils.streamConverter)((0, _utils.streamReducer)({
                stream,
                initialValue: "",
                reducer: (accumulator, part)=>accumulator += part.delta,
                finished: (accumulator)=>{
                    chatHistory.addMessage({
                        content: accumulator,
                        role: "assistant"
                    });
                }
            }), (r)=>new _Response.Response(r.delta, requestMessages.nodes));
        }
        const response = await this.chatModel.chat({
            messages: requestMessages.messages,
            parentEvent
        });
        chatHistory.addMessage(response.message);
        return new _Response.Response(response.message.content, requestMessages.nodes);
    }
    reset() {
        this.chatHistory.reset();
    }
    async prepareRequestMessages(message, chatHistory, parentEvent) {
        chatHistory.addMessage({
            content: message,
            role: "user"
        });
        const textOnly = (0, _utils.extractText)(message);
        const context = await this.contextGenerator.generate(textOnly, parentEvent);
        const nodes = context.nodes.map((r)=>r.node);
        const messages = await chatHistory.requestMessages(context ? [
            context.message
        ] : undefined);
        return {
            nodes,
            messages
        };
    }
}
