import { randomUUID } from "@llamaindex/env";
import { getHistory } from "../../ChatHistory.js";
import { Response } from "../../Response.js";
import { OpenAI } from "../../llm/index.js";
import { extractText, streamConverter, streamReducer } from "../../llm/utils.js";
import { PromptMixin } from "../../prompts/Mixin.js";
import { DefaultContextGenerator } from "./DefaultContextGenerator.js";
/**
 * ContextChatEngine uses the Index to get the appropriate context for each query.
 * The context is stored in the system prompt, and the chat history is preserved,
 * ideally allowing the appropriate context to be surfaced for each query.
 */ export class ContextChatEngine extends PromptMixin {
    chatModel;
    chatHistory;
    contextGenerator;
    constructor(init){
        super();
        this.chatModel = init.chatModel ?? new OpenAI({
            model: "gpt-3.5-turbo-16k"
        });
        this.chatHistory = getHistory(init?.chatHistory);
        this.contextGenerator = new DefaultContextGenerator({
            retriever: init.retriever,
            contextSystemPrompt: init?.contextSystemPrompt,
            nodePostprocessors: init?.nodePostprocessors
        });
    }
    _getPromptModules() {
        return {
            contextGenerator: this.contextGenerator
        };
    }
    async chat(params) {
        const { message, stream } = params;
        const chatHistory = params.chatHistory ? getHistory(params.chatHistory) : this.chatHistory;
        const parentEvent = {
            id: randomUUID(),
            type: "wrapper",
            tags: [
                "final"
            ]
        };
        const requestMessages = await this.prepareRequestMessages(message, chatHistory, parentEvent);
        if (stream) {
            const stream = await this.chatModel.chat({
                messages: requestMessages.messages,
                parentEvent,
                stream: true
            });
            return streamConverter(streamReducer({
                stream,
                initialValue: "",
                reducer: (accumulator, part)=>accumulator += part.delta,
                finished: (accumulator)=>{
                    chatHistory.addMessage({
                        content: accumulator,
                        role: "assistant"
                    });
                }
            }), (r)=>new Response(r.delta, requestMessages.nodes));
        }
        const response = await this.chatModel.chat({
            messages: requestMessages.messages,
            parentEvent
        });
        chatHistory.addMessage(response.message);
        return new Response(response.message.content, requestMessages.nodes);
    }
    reset() {
        this.chatHistory.reset();
    }
    async prepareRequestMessages(message, chatHistory, parentEvent) {
        chatHistory.addMessage({
            content: message,
            role: "user"
        });
        const textOnly = extractText(message);
        const context = await this.contextGenerator.generate(textOnly, parentEvent);
        const nodes = context.nodes.map((r)=>r.node);
        const messages = await chatHistory.requestMessages(context ? [
            context.message
        ] : undefined);
        return {
            nodes,
            messages
        };
    }
}
