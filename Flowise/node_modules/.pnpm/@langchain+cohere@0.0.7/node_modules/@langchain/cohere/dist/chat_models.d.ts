import { CohereClient, Cohere } from "cohere-ai";
import { type BaseMessage } from "@langchain/core/messages";
import { type BaseLanguageModelCallOptions } from "@langchain/core/language_models/base";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { type BaseChatModelParams, BaseChatModel } from "@langchain/core/language_models/chat_models";
import { ChatGenerationChunk, ChatResult } from "@langchain/core/outputs";
/**
 * Input interface for ChatCohere
 */
export interface ChatCohereInput extends BaseChatModelParams {
    /**
     * The API key to use.
     * @default {process.env.COHERE_API_KEY}
     */
    apiKey?: string;
    /**
     * The name of the model to use.
     * @default {"command"}
     */
    model?: string;
    /**
     * What sampling temperature to use, between 0.0 and 2.0.
     * Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
     * @default {0.3}
     */
    temperature?: number;
    /**
     * Whether or not to stream the response.
     * @default {false}
     */
    streaming?: boolean;
}
interface TokenUsage {
    completionTokens?: number;
    promptTokens?: number;
    totalTokens?: number;
}
interface CohereChatCallOptions extends BaseLanguageModelCallOptions, Partial<Omit<Cohere.ChatRequest, "message">>, Partial<Omit<Cohere.ChatStreamRequest, "message">> {
}
/**
 * Integration with ChatCohere
 * @example
 * ```typescript
 * const model = new ChatCohere({
 *   apiKey: process.env.COHERE_API_KEY, // Default
 *   model: "command" // Default
 * });
 * const response = await model.invoke([
 *   new HumanMessage("How tall are the largest pengiuns?")
 * ]);
 * ```
 */
export declare class ChatCohere<CallOptions extends CohereChatCallOptions = CohereChatCallOptions> extends BaseChatModel<CallOptions> implements ChatCohereInput {
    static lc_name(): string;
    lc_serializable: boolean;
    client: CohereClient;
    model: string;
    temperature: number;
    streaming: boolean;
    constructor(fields?: ChatCohereInput);
    _llmType(): string;
    invocationParams(options: this["ParsedCallOptions"]): {
        [k: string]: string | number | CallOptions["preamble"] | CallOptions["conversationId"] | CallOptions["promptTruncation"] | CallOptions["connectors"] | CallOptions["searchQueriesOnly"] | CallOptions["documents"] | undefined;
    };
    /** @ignore */
    _generate(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    _streamResponseChunks(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
    /** @ignore */
    _combineLLMOutput(...llmOutputs: CohereLLMOutput[]): CohereLLMOutput;
    get lc_secrets(): {
        [key: string]: string;
    } | undefined;
    get lc_aliases(): {
        [key: string]: string;
    } | undefined;
}
interface CohereLLMOutput {
    estimatedTokenUsage?: TokenUsage;
}
export {};
