/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSinkUpdateComponentOne = ChromaVectorStore | PineconeVectorStore | PgVectorStore | QdrantVectorStore | WeaviateVectorStore;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSinkUpdateComponent = Record<string, unknown> | DataSinkUpdateComponentOne;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

interface DataSinkUpdate {
    /** The name of the data sink. */
    name?: string;
    sinkType: ConfigurableDataSinkNames;
    component?: DataSinkUpdateComponent;
}

type index$9_DataSinkUpdate = DataSinkUpdate;
type index$9_DataSinkUpdateComponent = DataSinkUpdateComponent;
type index$9_DataSinkUpdateComponentOne = DataSinkUpdateComponentOne;
declare namespace index$9 {
  export type { index$9_DataSinkUpdate as DataSinkUpdate, index$9_DataSinkUpdateComponent as DataSinkUpdateComponent, index$9_DataSinkUpdateComponentOne as DataSinkUpdateComponentOne };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSourceUpdateComponentOne = DiscordReader | NotionPageReader | SlackReader | SimpleWebPageReader | TrafilaturaWebReader | BeautifulSoupWebReader | RssReader | YoutubeTranscriptReader | GoogleDocsReader | GoogleSheetsReader | ReaderConfig | DocumentGroup | TextNode | Document;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSourceUpdateComponent = Record<string, unknown> | DataSourceUpdateComponentOne | ExternallyStoredComponent;

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface BodyUpsertDataSourceFromFilesApiDataSourceFileUploadPut {
    files: string[];
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

interface DataSourceUpdate {
    /** The name of the data source. */
    name?: string;
    sourceType: ConfigurableDataSourceNames;
    component?: DataSourceUpdateComponent;
}

type index$8_BodyUpsertDataSourceFromFilesApiDataSourceFileUploadPut = BodyUpsertDataSourceFromFilesApiDataSourceFileUploadPut;
type index$8_DataSourceUpdate = DataSourceUpdate;
type index$8_DataSourceUpdateComponent = DataSourceUpdateComponent;
type index$8_DataSourceUpdateComponentOne = DataSourceUpdateComponentOne;
declare namespace index$8 {
  export type { index$8_BodyUpsertDataSourceFromFilesApiDataSourceFileUploadPut as BodyUpsertDataSourceFromFilesApiDataSourceFileUploadPut, index$8_DataSourceUpdate as DataSourceUpdate, index$8_DataSourceUpdateComponent as DataSourceUpdateComponent, index$8_DataSourceUpdateComponentOne as DataSourceUpdateComponentOne };
}

declare namespace index$7 {
  export {  };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface ApiKeyCreate {
    name?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface ApiKeyUpdate {
    name?: string;
}

type index$6_ApiKeyCreate = ApiKeyCreate;
type index$6_ApiKeyUpdate = ApiKeyUpdate;
declare namespace index$6 {
  export type { index$6_ApiKeyCreate as ApiKeyCreate, index$6_ApiKeyUpdate as ApiKeyUpdate };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {}
 */
interface ListProjectsApiProjectGetRequest {
    projectName?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface ProjectUpdate {
    name: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface EvalDatasetCreate {
    /** The name of the EvalDataset. */
    name: string;
}

type index$5_EvalDatasetCreate = EvalDatasetCreate;
type index$5_ListProjectsApiProjectGetRequest = ListProjectsApiProjectGetRequest;
type index$5_ProjectUpdate = ProjectUpdate;
declare namespace index$5 {
  export type { index$5_EvalDatasetCreate as EvalDatasetCreate, index$5_ListProjectsApiProjectGetRequest as ListProjectsApiProjectGetRequest, index$5_ProjectUpdate as ProjectUpdate };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * @example
 *     {
 *         projectName: "project-name",
 *         pipelineType: PlatformApi.PipelineType.Playground
 *     }
 */
interface SearchPipelinesApiPipelineGetRequest {
    projectName: string;
    pipelineName?: string;
    pipelineType?: PipelineType;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface GetPipelineForProjectApiPipelinePipelineIdGetRequest {
    withManagedIngestionStatus?: boolean;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

interface PipelineUpdate {
    configuredTransformations?: ConfiguredTransformationItem[];
    /** List of data source IDs. When provided instead of data_sources, the data sources will be looked up by ID. */
    dataSourceIds?: string[];
    /** List of data sources. When provided instead of data_source_ids, the data sources will be created. */
    dataSources?: DataSourceCreate[];
    /** List of data sink IDs. When provided instead of data_sinks, the data sinks will be looked up by ID. */
    dataSinkIds?: string[];
    /** List of data sinks. When provided instead of data_sink_ids, the data sinks will be created. */
    dataSinks?: DataSinkCreate[];
    /** Preset retrieval parameters for the pipeline. */
    presetRetrievalParameters?: PresetRetrievalParams;
    /** Eval parameters for the pipeline. */
    evalParameters?: EvalExecutionParams;
    name?: string;
    /** The ID of the ManagedPipeline this playground pipeline is linked to. */
    managedPipelineId?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest {
    managedPipelineName?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest {
    playgroundPipelineName?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {}
 */
interface CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest {
    loadedFileIds?: string | string[];
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {}
 */
interface GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest {
    configuredTransformationId?: string;
    loadedFileId?: string;
    offset?: number;
    limit?: number;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * @example
 *     {
 *         evalQuestionIds: [],
 *         params: {
 *             llmModel: PlatformApi.SupportedEvalLlmModelNames.Gpt35Turbo
 *         }
 *     }
 */
interface EvalExecutionCreate {
    evalQuestionIds: string[];
    /** The parameters for the eval execution that will override the ones set in the pipeline. */
    params?: EvalExecutionParamsOverride;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface BodyRunManagedRawFilesIngestionApiPipelinePipelineIdManagedIngestRawFilesPut {
    files: string[];
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface RetrievalParams {
    /** Number of nodes for dense retrieval. */
    denseSimilarityTopK?: number;
    /** Number of nodes for sparse retrieval. */
    sparseSimilarityTopK?: number;
    /** Enable reranking for retrieval */
    enableReranking?: boolean;
    /** Number of reranked nodes for returning. */
    rerankTopN?: number;
    /** Alpha value for hybrid retrieval to determine the weights between dense and sparse retrieval. 0 is sparse retrieval and 1 is dense retrieval. */
    alpha?: number;
    /** Search filters for retrieval. the format of search_filters is a dict of {key: (operator, value)} */
    searchFilters?: Record<string, unknown[]>;
    /** The query to retrieve against. */
    query: string;
    className?: string;
}

type index$4_BodyRunManagedRawFilesIngestionApiPipelinePipelineIdManagedIngestRawFilesPut = BodyRunManagedRawFilesIngestionApiPipelinePipelineIdManagedIngestRawFilesPut;
type index$4_CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest = CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest;
type index$4_CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest = CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest;
type index$4_DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest = DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest;
type index$4_EvalExecutionCreate = EvalExecutionCreate;
type index$4_GetPipelineForProjectApiPipelinePipelineIdGetRequest = GetPipelineForProjectApiPipelinePipelineIdGetRequest;
type index$4_GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest = GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest;
type index$4_PipelineUpdate = PipelineUpdate;
type index$4_RetrievalParams = RetrievalParams;
type index$4_SearchPipelinesApiPipelineGetRequest = SearchPipelinesApiPipelineGetRequest;
declare namespace index$4 {
  export type { index$4_BodyRunManagedRawFilesIngestionApiPipelinePipelineIdManagedIngestRawFilesPut as BodyRunManagedRawFilesIngestionApiPipelinePipelineIdManagedIngestRawFilesPut, index$4_CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest as CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest, index$4_CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest as CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest, index$4_DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest as DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest, index$4_EvalExecutionCreate as EvalExecutionCreate, index$4_GetPipelineForProjectApiPipelinePipelineIdGetRequest as GetPipelineForProjectApiPipelinePipelineIdGetRequest, index$4_GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest as GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest, index$4_PipelineUpdate as PipelineUpdate, index$4_RetrievalParams as RetrievalParams, index$4_SearchPipelinesApiPipelineGetRequest as SearchPipelinesApiPipelineGetRequest };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface EvalDatasetUpdate {
    /** The name of the EvalDataset. */
    name: string;
}

type index$3_EvalDatasetUpdate = EvalDatasetUpdate;
declare namespace index$3 {
  export type { index$3_EvalDatasetUpdate as EvalDatasetUpdate };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

interface BodyUploadFileApiParsingUploadPost {
    language?: ParserLanguages | ParserLanguages[];
}

type index$2_BodyUploadFileApiParsingUploadPost = BodyUploadFileApiParsingUploadPost;
declare namespace index$2 {
  export type { index$2_BodyUploadFileApiParsingUploadPost as BodyUploadFileApiParsingUploadPost };
}

declare namespace index$1 {
  export {  };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Schema for an API Key.
 */
interface ApiKey {
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    name?: string;
    userId: string;
    redactedApiKey: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * OpenAI class for embeddings.
 *
 * Args:
 * mode (str): Mode for embedding.
 * Defaults to OpenAIEmbeddingMode.TEXT_SEARCH_MODE.
 * Options are:
 *
 *         - OpenAIEmbeddingMode.SIMILARITY_MODE
 *         - OpenAIEmbeddingMode.TEXT_SEARCH_MODE
 *
 *     model (str): Model for embedding.
 *         Defaults to OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002.
 *         Options are:
 *
 *         - OpenAIEmbeddingModelType.DAVINCI
 *         - OpenAIEmbeddingModelType.CURIE
 *         - OpenAIEmbeddingModelType.BABBAGE
 *         - OpenAIEmbeddingModelType.ADA
 *         - OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002
 */
interface AzureOpenAiEmbedding {
    /** The name of the embedding model. */
    modelName?: string;
    /** The batch size for embedding calls. */
    embedBatchSize?: number;
    callbackManager?: Record<string, unknown>;
    /** Additional kwargs for the OpenAI API. */
    additionalKwargs?: Record<string, unknown>;
    /** The OpenAI API key. */
    apiKey: string;
    /** The base URL for OpenAI API. */
    apiBase: string;
    /** The version for OpenAI API. */
    apiVersion: string;
    /** Maximum number of retries. */
    maxRetries?: number;
    /** Timeout for each request. */
    timeout?: number;
    /** The default headers for API requests. */
    defaultHeaders?: Record<string, string>;
    /** Reuse the OpenAI client between requests. When doing anything with large volumes of async API calls, setting this to false can improve stability. */
    reuseClient?: boolean;
    /** The number of dimensions on the output embedding vectors. Works only with v3 embedding models. */
    dimensions?: number;
    /** The Azure endpoint to use. */
    azureEndpoint?: string;
    /** The Azure deployment to use. */
    azureDeployment?: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Serialiable Data Loader with Pydatnic.
 */
interface BasePydanticReader {
    /** Whether the data is loaded from a remote API or a local file. */
    isRemote?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * BeautifulSoup web page reader.
 *
 * Reads pages from the web.
 * Requires the `bs4` and `urllib` packages.
 *
 * Args:
 * website_extractor (Optional[Dict[str, Callable]]): A mapping of website
 * hostname (e.g. google.com) to a function that specifies how to
 * extract text from the BeautifulSoup obj. See DEFAULT_WEBSITE_EXTRACTOR.
 */
interface BeautifulSoupWebReader {
    isRemote?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Chroma vector store.
 *
 * In this vector store, embeddings are stored within a ChromaDB collection.
 *
 * During query time, the index uses ChromaDB to query for the top
 * k most similar nodes.
 *
 * Args:
 * chroma_collection (chromadb.api.models.Collection.Collection):
 * ChromaDB collection instance
 */
interface ChromaVectorStore {
    storesText?: boolean;
    isEmbeddingQuery?: boolean;
    flatMetadata?: boolean;
    collectionName?: string;
    host?: string;
    port?: string;
    ssl: boolean;
    headers?: Record<string, string>;
    persistDir?: string;
    collectionKwargs?: Record<string, unknown>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Split code using a AST parser.
 *
 * Thank you to Kevin Lu / SweepAI for suggesting this elegant code splitting solution.
 * https://docs.sweep.dev/blogs/chunking-2m-files
 */
interface CodeSplitter {
    /** Whether or not to consider metadata when splitting. */
    includeMetadata?: boolean;
    /** Include prev/next node relationships. */
    includePrevNextRel?: boolean;
    callbackManager?: Record<string, unknown>;
    /** The programming language of the code being split. */
    language: string;
    /** The number of lines to include in each chunk. */
    chunkLines?: number;
    /** How many lines of code each chunk overlaps with. */
    chunkLinesOverlap?: number;
    /** Maximum number of characters per chunk. */
    maxChars?: number;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * An enumeration.
 */
type ConfigurableDataSinkNames = "CHROMA" | "PINECONE" | "POSTGRES" | "QDRANT" | "WEAVIATE";
declare const ConfigurableDataSinkNames: {
    readonly Chroma: "CHROMA";
    readonly Pinecone: "PINECONE";
    readonly Postgres: "POSTGRES";
    readonly Qdrant: "QDRANT";
    readonly Weaviate: "WEAVIATE";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * An enumeration.
 */
type ConfigurableDataSourceNames = "DISCORD" | "NOTION_PAGE" | "SLACK" | "SIMPLE_WEB_PAGE" | "TRAFILATURA_WEB_PAGE" | "BEAUTIFUL_SOUP_WEB_PAGE" | "RSS" | "YOUTUBE_TRANSCRIPT" | "GOOGLE_DOCS" | "GOOGLE_SHEETS" | "READER" | "DOCUMENT_GROUP" | "TEXT_NODE" | "DOCUMENT";
declare const ConfigurableDataSourceNames: {
    readonly Discord: "DISCORD";
    readonly NotionPage: "NOTION_PAGE";
    readonly Slack: "SLACK";
    readonly SimpleWebPage: "SIMPLE_WEB_PAGE";
    readonly TrafilaturaWebPage: "TRAFILATURA_WEB_PAGE";
    readonly BeautifulSoupWebPage: "BEAUTIFUL_SOUP_WEB_PAGE";
    readonly Rss: "RSS";
    readonly YoutubeTranscript: "YOUTUBE_TRANSCRIPT";
    readonly GoogleDocs: "GOOGLE_DOCS";
    readonly GoogleSheets: "GOOGLE_SHEETS";
    readonly Reader: "READER";
    readonly DocumentGroup: "DOCUMENT_GROUP";
    readonly TextNode: "TEXT_NODE";
    readonly Document: "DOCUMENT";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a transformation definition.
 */
interface ConfigurableTransformationDefinition {
    /** The label field will be used to display the name of the component in the UI */
    label: string;
    /** The json_schema field can be used by clients to determine how to construct the component */
    jsonSchema: Record<string, unknown>;
    /** The name field will act as the unique identifier of TransformationDefinition objects */
    configurableTransformationType: ConfigurableTransformationNames;
    /** The transformation_category field will be used to group transformations in the UI */
    transformationCategory: TransformationCategoryNames;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * An enumeration.
 */
type ConfigurableTransformationNames = "CODE_NODE_PARSER" | "SENTENCE_AWARE_NODE_PARSER" | "TOKEN_AWARE_NODE_PARSER" | "HTML_NODE_PARSER" | "MARKDOWN_NODE_PARSER" | "JSON_NODE_PARSER" | "SIMPLE_FILE_NODE_PARSER" | "OPENAI_EMBEDDING" | "AZURE_EMBEDDING";
declare const ConfigurableTransformationNames: {
    readonly CodeNodeParser: "CODE_NODE_PARSER";
    readonly SentenceAwareNodeParser: "SENTENCE_AWARE_NODE_PARSER";
    readonly TokenAwareNodeParser: "TOKEN_AWARE_NODE_PARSER";
    readonly HtmlNodeParser: "HTML_NODE_PARSER";
    readonly MarkdownNodeParser: "MARKDOWN_NODE_PARSER";
    readonly JsonNodeParser: "JSON_NODE_PARSER";
    readonly SimpleFileNodeParser: "SIMPLE_FILE_NODE_PARSER";
    readonly OpenaiEmbedding: "OPENAI_EMBEDDING";
    readonly AzureEmbedding: "AZURE_EMBEDDING";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type ConfiguredTransformationItemComponentOne = CodeSplitter | SentenceSplitter | TokenTextSplitter | HtmlNodeParser | MarkdownNodeParser | JsonNodeParser | SimpleFileNodeParser | OpenAiEmbedding | AzureOpenAiEmbedding;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Component that implements the transformation
 */
type ConfiguredTransformationItemComponent = Record<string, unknown> | ConfiguredTransformationItemComponentOne;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Configured transformations for pipelines.
 *
 * Similar to ConfigurableTransformation but includes a few
 * more fields that are useful to the platform.
 */
interface ConfiguredTransformationItem {
    id?: string;
    /** Name for the type of transformation this is (e.g. SIMPLE_NODE_PARSER). Can also be an enum instance of llama_index.ingestion.transformations.ConfigurableTransformations. This will be converted to ConfigurableTransformationNames. */
    configurableTransformationType: ConfigurableTransformationNames;
    /** Component that implements the transformation */
    component: ConfiguredTransformationItemComponent;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSinkComponentOne = ChromaVectorStore | PineconeVectorStore | PgVectorStore | QdrantVectorStore | WeaviateVectorStore;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSinkComponent = Record<string, unknown> | DataSinkComponentOne;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a data sink.
 */
interface DataSink {
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The name of the data sink. */
    name: string;
    sinkType: ConfigurableDataSinkNames;
    component: DataSinkComponent;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSinkCreateComponentOne = ChromaVectorStore | PineconeVectorStore | PgVectorStore | QdrantVectorStore | WeaviateVectorStore;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSinkCreateComponent = Record<string, unknown> | DataSinkCreateComponentOne;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for creating a data sink.
 */
interface DataSinkCreate {
    /** The name of the data sink. */
    name: string;
    sinkType: ConfigurableDataSinkNames;
    component: DataSinkCreateComponent;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a data sink definition.
 */
interface DataSinkDefinition {
    /** The label field will be used to display the name of the component in the UI */
    label: string;
    /** The json_schema field can be used by clients to determine how to construct the component */
    jsonSchema: Record<string, unknown>;
    /** The name field will act as the unique identifier of DataSinkDefinition objects */
    sinkType: ConfigurableDataSinkNames;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSourceComponentOne = DiscordReader | NotionPageReader | SlackReader | SimpleWebPageReader | TrafilaturaWebReader | BeautifulSoupWebReader | RssReader | YoutubeTranscriptReader | GoogleDocsReader | GoogleSheetsReader | ReaderConfig | DocumentGroup | TextNode | Document;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSourceComponent = Record<string, unknown> | DataSourceComponentOne | ExternallyStoredComponent;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a data source.
 */
interface DataSource {
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The name of the data source. */
    name: string;
    sourceType: ConfigurableDataSourceNames;
    component: DataSourceComponent;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSourceCreateComponentOne = DiscordReader | NotionPageReader | SlackReader | SimpleWebPageReader | TrafilaturaWebReader | BeautifulSoupWebReader | RssReader | YoutubeTranscriptReader | GoogleDocsReader | GoogleSheetsReader | ReaderConfig | DocumentGroup | TextNode | Document;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSourceCreateComponent = Record<string, unknown> | DataSourceCreateComponentOne | ExternallyStoredComponent;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for creating a data source.
 */
interface DataSourceCreate {
    /** The name of the data source. */
    name: string;
    sourceType: ConfigurableDataSourceNames;
    component: DataSourceCreateComponent;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a data source definition.
 */
interface DataSourceDefinition {
    /** The label field will be used to display the name of the component in the UI */
    label: string;
    /** The json_schema field can be used by clients to determine how to construct the component */
    jsonSchema: Record<string, unknown>;
    /** The name field will act as the unique identifier of DataSourceDefinition objects */
    sourceType: ConfigurableDataSourceNames;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for job that loads from a data source.
 */
interface DataSourceLoadJobRecord {
    /** Unique identifier */
    id?: string;
    jobName?: JobNames;
    status: StatusEnum;
    startedAt?: Date;
    endedAt?: Date;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The partitions for this execution. */
    partitions: Record<string, string>;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for job that executes managed pipeline ingestion over a single data-source linked to a pipeline.
 */
interface DataSourceManagedIngestionJobRecord {
    /** Unique identifier */
    id?: string;
    jobName: JobNames;
    status: StatusEnum;
    startedAt?: Date;
    endedAt?: Date;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The partitions for this execution. */
    partitions: Record<string, string>;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Discord reader.
 *
 * Reads conversations from channels.
 *
 * Args:
 * discord_token (Optional[str]): Discord token. If not provided, we
 * assume the environment variable `DISCORD_TOKEN` is set.
 */
interface DiscordReader {
    isRemote?: boolean;
    discordToken: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DocumentRelationshipsValue = RelatedNodeInfo | RelatedNodeInfo[];

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Generic interface for a data document.
 *
 * This document connects to data sources.
 */
interface Document {
    /** Unique ID of the node. */
    docId?: string;
    /** Embedding of the node. */
    embedding?: number[];
    /** A flat dictionary of metadata fields */
    extraInfo?: Record<string, unknown>;
    /** Metadata keys that are excluded from text for the embed model. */
    excludedEmbedMetadataKeys?: string[];
    /** Metadata keys that are excluded from text for the LLM. */
    excludedLlmMetadataKeys?: string[];
    /** A mapping of relationships to other node information. */
    relationships?: Record<string, DocumentRelationshipsValue>;
    /** Text content of the node. */
    text?: string;
    /** Start char index of the node. */
    startCharIdx?: number;
    /** End char index of the node. */
    endCharIdx?: number;
    /** Template for how text is formatted, with {content} and {metadata_str} placeholders. */
    textTemplate?: string;
    /** Template for how metadata is formatted, with {key} and {value} placeholders. */
    metadataTemplate?: string;
    /** Separator between metadata fields when converting to string. */
    metadataSeperator?: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * A group of documents, usually separate pages from a single file.
 */
interface DocumentGroup {
    /** Whether the data is loaded from a remote API or a local file. */
    isRemote?: boolean;
    /** Path to the file containing the documents */
    filePath: string;
    /** Sequential group of documents, usually separate pages from a single file. */
    documents: Document[];
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Schema for an eval dataset.
 * Includes the other DB fields like id, created_at, & updated_at.
 */
interface EvalDataset {
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The name of the EvalDataset. */
    name: string;
    projectId: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for job that evaluates an EvalDataset against a pipeline.
 */
interface EvalDatasetJobRecord {
    /** Unique identifier */
    id?: string;
    jobName: JobNames;
    status: StatusEnum;
    startedAt?: Date;
    endedAt?: Date;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The partitions for this execution. */
    partitions: Record<string, string>;
    /** The IDs for the EvalQuestions this execution ran against. */
    evalQuestionIds: string[];
    /** The parameters for the eval execution. */
    evalExecutionParams: EvalExecutionParams;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for the params for an eval execution.
 */
interface EvalExecutionParams {
    /** The LLM model to use within eval execution. */
    llmModel?: SupportedEvalLlmModelNames;
    /** The template to use for the question answering prompt. */
    qaPromptTmpl?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for the params override for an eval execution.
 */
interface EvalExecutionParamsOverride {
    /** The LLM model to use within eval execution. */
    llmModel?: SupportedEvalLlmModelNames;
    /** The template to use for the question answering prompt. */
    qaPromptTmpl?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Schema for an eval LLM model.
 */
interface EvalLlmModelData {
    /** The name of the LLM model. */
    name: string;
    /** The description of the LLM model. */
    description: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Base schema model containing common database fields.
 */
interface EvalQuestion {
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The content of the question. */
    content: string;
    evalDatasetId: string;
    /** The index at which this question is positioned relative to the other questions in the linked EvalDataset. Client is responsible for setting this correctly. */
    evalDatasetIndex: number;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface EvalQuestionCreate {
    /** The content of the question. */
    content: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for the result of an eval question job.
 */
interface EvalQuestionResult {
    /** The ID of the question that was executed. */
    evalQuestionId: string;
    /** The ID of the pipeline that the question was executed against. */
    pipelineId: string;
    /** The nodes retrieved by the pipeline for the given question. */
    sourceNodes: TextNode[];
    /** The answer to the question. */
    answer: string;
    /** The eval metrics for the question. */
    evalMetrics: Record<string, MetricResult>;
    /** The ID of the EvalDatasetJobRecord that this result was generated from. */
    evalDatasetExecutionId: string;
    /** The EvalExecutionParams that were used when this result was generated. */
    evalDatasetExecutionParams: EvalExecutionParams;
    /** The timestamp when the eval finished. */
    evalFinishedAt: Date;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Base schema model for BaseComponent classes used in the platform.
 * Comes with special serialization logic for types used commonly in platform codebase.
 */
interface ExternallyStoredComponent {
    /** The ID of the externally stored component. */
    id: string;
    /** The extra path prefix for the externally stored component. */
    extraPathPrefix: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Google Docs reader.
 *
 * Reads a page from Google Docs
 */
interface GoogleDocsReader {
    isRemote?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Google Sheets reader.
 *
 * Reads a sheet as TSV from Google Sheets
 */
interface GoogleSheetsReader {
    isRemote?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * HTML node parser.
 *
 * Splits a document into Nodes using custom HTML splitting logic.
 *
 * Args:
 * include_metadata (bool): whether to include metadata in nodes
 * include_prev_next_rel (bool): whether to include prev/next relationships
 */
interface HtmlNodeParser {
    /** Whether or not to consider metadata when splitting. */
    includeMetadata?: boolean;
    /** Include prev/next node relationships. */
    includePrevNextRel?: boolean;
    callbackManager?: Record<string, unknown>;
    /** HTML tags to extract text from. */
    tags?: string[];
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

interface HttpValidationError {
    detail?: ValidationError[];
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * JSON node parser.
 *
 * Splits a document into Nodes using custom JSON splitting logic.
 *
 * Args:
 * include_metadata (bool): whether to include metadata in nodes
 * include_prev_next_rel (bool): whether to include prev/next relationships
 */
interface JsonNodeParser {
    /** Whether or not to consider metadata when splitting. */
    includeMetadata?: boolean;
    /** Include prev/next node relationships. */
    includePrevNextRel?: boolean;
    callbackManager?: Record<string, unknown>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Enum for executable pipeline job names.
 */
type JobNames = "load_documents_job" | "playground_job" | "eval_dataset_job" | "pipeline_managed_ingestion_job" | "data_source_managed_ingestion_job" | "loaded_file_managed_ingestion_job";
declare const JobNames: {
    readonly LoadDocumentsJob: "load_documents_job";
    readonly PlaygroundJob: "playground_job";
    readonly EvalDatasetJob: "eval_dataset_job";
    readonly PipelineManagedIngestionJob: "pipeline_managed_ingestion_job";
    readonly DataSourceManagedIngestionJob: "data_source_managed_ingestion_job";
    readonly LoadedFileManagedIngestionJob: "loaded_file_managed_ingestion_job";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Schema for reading a loaded file.
 */
interface LoadedFile {
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    name: string;
    /** Size of the file in bytes */
    fileSize?: number;
    /** File type (e.g. PDF, DOCX, etc.) */
    fileType?: string;
    /** Number of pages in the file */
    numPages?: number;
    dataSourceId: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for loaded file contents that are stored in blob storage (S3).
 */
interface LoadedFilePayload {
    loadedFileId: string;
    documents?: Document[];
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Status of managed ingestion.
 */
type ManagedIngestionStatus = "NOT_STARTED" | "CREATING" | "IN_PROGRESS" | "SUCCESS" | "ERROR";
declare const ManagedIngestionStatus: {
    readonly NotStarted: "NOT_STARTED";
    readonly Creating: "CREATING";
    readonly InProgress: "IN_PROGRESS";
    readonly Success: "SUCCESS";
    readonly Error: "ERROR";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Markdown node parser.
 *
 * Splits a document into Nodes using custom Markdown splitting logic.
 *
 * Args:
 * include_metadata (bool): whether to include metadata in nodes
 * include_prev_next_rel (bool): whether to include prev/next relationships
 */
interface MarkdownNodeParser {
    /** Whether or not to consider metadata when splitting. */
    includeMetadata?: boolean;
    /** Include prev/next node relationships. */
    includePrevNextRel?: boolean;
    callbackManager?: Record<string, unknown>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface MetricResult {
    /** Whether the metric passed or not. */
    passing?: boolean;
    /** The score for the metric. */
    score?: number;
    /** The reasoning for the metric. */
    feedback?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Notion Page reader.
 *
 * Reads a set of Notion pages.
 *
 * Args:
 * integration_token (str): Notion integration token.
 */
interface NotionPageReader {
    isRemote?: boolean;
    token: string;
    headers: Record<string, string>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * An enumeration.
 */
type ObjectType = "1" | "2" | "3" | "4";
declare const ObjectType: {
    readonly One: "1";
    readonly Two: "2";
    readonly Three: "3";
    readonly Four: "4";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * OpenAI class for embeddings.
 *
 * Args:
 * mode (str): Mode for embedding.
 * Defaults to OpenAIEmbeddingMode.TEXT_SEARCH_MODE.
 * Options are:
 *
 *         - OpenAIEmbeddingMode.SIMILARITY_MODE
 *         - OpenAIEmbeddingMode.TEXT_SEARCH_MODE
 *
 *     model (str): Model for embedding.
 *         Defaults to OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002.
 *         Options are:
 *
 *         - OpenAIEmbeddingModelType.DAVINCI
 *         - OpenAIEmbeddingModelType.CURIE
 *         - OpenAIEmbeddingModelType.BABBAGE
 *         - OpenAIEmbeddingModelType.ADA
 *         - OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002
 */
interface OpenAiEmbedding {
    /** The name of the embedding model. */
    modelName?: string;
    /** The batch size for embedding calls. */
    embedBatchSize?: number;
    callbackManager?: Record<string, unknown>;
    /** Additional kwargs for the OpenAI API. */
    additionalKwargs?: Record<string, unknown>;
    /** The OpenAI API key. */
    apiKey: string;
    /** The base URL for OpenAI API. */
    apiBase: string;
    /** The version for OpenAI API. */
    apiVersion: string;
    /** Maximum number of retries. */
    maxRetries?: number;
    /** Timeout for each request. */
    timeout?: number;
    /** The default headers for API requests. */
    defaultHeaders?: Record<string, string>;
    /** Reuse the OpenAI client between requests. When doing anything with large volumes of async API calls, setting this to false can improve stability. */
    reuseClient?: boolean;
    /** The number of dimensions on the output embedding vectors. Works only with v3 embedding models. */
    dimensions?: number;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Abstract vector store protocol.
 */
interface PgVectorStore {
    storesText?: boolean;
    isEmbeddingQuery?: boolean;
    connectionString: string;
    asyncConnectionString: string;
    tableName: string;
    schemaName: string;
    embedDim: number;
    hybridSearch: boolean;
    textSearchConfig: string;
    cacheOk: boolean;
    performSetup: boolean;
    debug: boolean;
    useJsonb: boolean;
    flatMetadata?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Enum for representing the languages supported by the parser
 */
type ParserLanguages = "af" | "az" | "bs" | "cs" | "cy" | "da" | "de" | "en" | "es" | "et" | "fr" | "ga" | "hr" | "hu" | "id" | "is" | "it" | "ku" | "la" | "lt" | "lv" | "mi" | "ms" | "mt" | "nl" | "no" | "oc" | "pi" | "pl" | "pt" | "ro" | "rs_latin" | "sk" | "sl" | "sq" | "sv" | "sw" | "tl" | "tr" | "uz" | "vi" | "ar" | "fa" | "ug" | "ur" | "bn" | "as" | "mni" | "ru" | "rs_cyrillic" | "be" | "bg" | "uk" | "mn" | "abq" | "ady" | "kbd" | "ava" | "dar" | "inh" | "che" | "lbe" | "lez" | "tab" | "tjk" | "hi" | "mr" | "ne" | "bh" | "mai" | "ang" | "bho" | "mah" | "sck" | "new" | "gom" | "sa" | "bgc" | "th" | "ch_sim" | "ch_tra" | "ja" | "ko" | "ta" | "te" | "kn";
declare const ParserLanguages: {
    readonly Af: "af";
    readonly Az: "az";
    readonly Bs: "bs";
    readonly Cs: "cs";
    readonly Cy: "cy";
    readonly Da: "da";
    readonly De: "de";
    readonly En: "en";
    readonly Es: "es";
    readonly Et: "et";
    readonly Fr: "fr";
    readonly Ga: "ga";
    readonly Hr: "hr";
    readonly Hu: "hu";
    readonly Id: "id";
    readonly Is: "is";
    readonly It: "it";
    readonly Ku: "ku";
    readonly La: "la";
    readonly Lt: "lt";
    readonly Lv: "lv";
    readonly Mi: "mi";
    readonly Ms: "ms";
    readonly Mt: "mt";
    readonly Nl: "nl";
    readonly No: "no";
    readonly Oc: "oc";
    readonly Pi: "pi";
    readonly Pl: "pl";
    readonly Pt: "pt";
    readonly Ro: "ro";
    readonly RsLatin: "rs_latin";
    readonly Sk: "sk";
    readonly Sl: "sl";
    readonly Sq: "sq";
    readonly Sv: "sv";
    readonly Sw: "sw";
    readonly Tl: "tl";
    readonly Tr: "tr";
    readonly Uz: "uz";
    readonly Vi: "vi";
    readonly Ar: "ar";
    readonly Fa: "fa";
    readonly Ug: "ug";
    readonly Ur: "ur";
    readonly Bn: "bn";
    readonly As: "as";
    readonly Mni: "mni";
    readonly Ru: "ru";
    readonly RsCyrillic: "rs_cyrillic";
    readonly Be: "be";
    readonly Bg: "bg";
    readonly Uk: "uk";
    readonly Mn: "mn";
    readonly Abq: "abq";
    readonly Ady: "ady";
    readonly Kbd: "kbd";
    readonly Ava: "ava";
    readonly Dar: "dar";
    readonly Inh: "inh";
    readonly Che: "che";
    readonly Lbe: "lbe";
    readonly Lez: "lez";
    readonly Tab: "tab";
    readonly Tjk: "tjk";
    readonly Hi: "hi";
    readonly Mr: "mr";
    readonly Ne: "ne";
    readonly Bh: "bh";
    readonly Mai: "mai";
    readonly Ang: "ang";
    readonly Bho: "bho";
    readonly Mah: "mah";
    readonly Sck: "sck";
    readonly New: "new";
    readonly Gom: "gom";
    readonly Sa: "sa";
    readonly Bgc: "bgc";
    readonly Th: "th";
    readonly ChSim: "ch_sim";
    readonly ChTra: "ch_tra";
    readonly Ja: "ja";
    readonly Ko: "ko";
    readonly Ta: "ta";
    readonly Te: "te";
    readonly Kn: "kn";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */

interface ParsingJob {
    id: string;
    status: StatusEnum;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface ParsingJobMarkdownResult {
    /** The markdown result of the parsing job */
    markdown: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface ParsingJobTextResult {
    /** The text result of the parsing job */
    text: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface ParsingUsage {
    usagePdfPages: number;
    maxPdfPages: number;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Pinecone Vector Store.
 *
 * In this vector store, embeddings and docs are stored within a
 * Pinecone index.
 *
 * During query time, the index uses Pinecone to query for the top
 * k most similar nodes.
 *
 * Args:
 * pinecone_index (Optional[Union[pinecone.Pinecone.Index, pinecone.Index]]): Pinecone index instance,
 * pinecone.Pinecone.Index for clients >= 3.0.0; pinecone.Index for older clients.
 * insert_kwargs (Optional[Dict]): insert kwargs during `upsert` call.
 * add_sparse_vector (bool): whether to add sparse vector to index.
 * tokenizer (Optional[Callable]): tokenizer to use to generate sparse
 * default_empty_query_vector (Optional[List[float]]): default empty query vector.
 * Defaults to None. If not None, then this vector will be used as the query
 * vector if the query is empty.
 */
interface PineconeVectorStore {
    storesText?: boolean;
    isEmbeddingQuery?: boolean;
    flatMetadata?: boolean;
    apiKey?: string;
    indexName?: string;
    environment?: string;
    namespace?: string;
    insertKwargs?: Record<string, unknown>;
    addSparseVector: boolean;
    textKey: string;
    batchSize: number;
    removeTextFromMetadata: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a pipeline.
 */
interface Pipeline {
    configuredTransformations: ConfiguredTransformationItem[];
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    name: string;
    projectId: string;
    /** Type of pipeline. Either PLAYGROUND or MANAGED. */
    pipelineType?: PipelineType;
    /** The ID of the ManagedPipeline this playground pipeline is linked to. */
    managedPipelineId?: string;
    /** Preset retrieval parameters for the pipeline. */
    presetRetrievalParameters?: PresetRetrievalParams;
    /** Eval parameters for the pipeline. */
    evalParameters?: EvalExecutionParams;
    /** Status of Managed Ingestion. */
    managedIngestionStatus?: ManagedIngestionStatus;
    dataSources: DataSource[];
    dataSinks: DataSink[];
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for creating a pipeline.
 */
interface PipelineCreate {
    /** List of configured transformations. */
    configuredTransformations?: ConfiguredTransformationItem[];
    /** List of data source IDs. When provided instead of data_sources, the data sources will be looked up by ID. */
    dataSourceIds?: string[];
    /** List of data sources. When provided instead of data_source_ids, the data sources will be created. */
    dataSources?: DataSourceCreate[];
    /** List of data sink IDs. When provided instead of data_sinks, the data sinks will be looked up by ID. */
    dataSinkIds?: string[];
    /** List of data sinks. When provided instead of data_sink_ids, the data sinks will be created. */
    dataSinks?: DataSinkCreate[];
    /** Preset retrieval parameters for the pipeline. */
    presetRetrievalParameters?: PresetRetrievalParams;
    /** Eval parameters for the pipeline. */
    evalParameters?: EvalExecutionParams;
    name: string;
    /** Type of pipeline. Either PLAYGROUND or MANAGED. */
    pipelineType?: PipelineType;
    /** The ID of the ManagedPipeline this playground pipeline is linked to. */
    managedPipelineId?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for job that executes managed pipeline ingestion over all data-sources linked to a pipeline.
 */
interface PipelineManagedIngestionJobRecord {
    /** Unique identifier */
    id?: string;
    jobName: JobNames;
    status: StatusEnum;
    startedAt?: Date;
    endedAt?: Date;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The partitions for this execution. */
    partitions: Record<string, string>;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Enum for representing the type of a pipeline
 */
type PipelineType = "PLAYGROUND" | "MANAGED";
declare const PipelineType: {
    readonly Playground: "PLAYGROUND";
    readonly Managed: "MANAGED";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type PlatformTextNodeRelationshipsValue = RelatedNodeInfo | RelatedNodeInfo[];

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Base node Object.
 *
 * Generic abstract interface for retrievable nodes
 */
interface PlatformTextNode {
    /** Unique ID of the node. */
    id?: string;
    /** Embedding of the node. */
    embedding?: number[];
    /** A flat dictionary of metadata fields */
    extraInfo?: Record<string, unknown>;
    /** Metadata keys that are excluded from text for the embed model. */
    excludedEmbedMetadataKeys?: string[];
    /** Metadata keys that are excluded from text for the LLM. */
    excludedLlmMetadataKeys?: string[];
    /** A mapping of relationships to other node information. */
    relationships?: Record<string, PlatformTextNodeRelationshipsValue>;
    /** Text content of the node. */
    text?: string;
    /** Start char index of the node. */
    startCharIdx?: number;
    /** End char index of the node. */
    endCharIdx?: number;
    /** Template for how text is formatted, with {content} and {metadata_str} placeholders. */
    textTemplate?: string;
    /** Template for how metadata is formatted, with {key} and {value} placeholders. */
    metadataTemplate?: string;
    /** Separator between metadata fields when converting to string. */
    metadataSeperator?: string;
    /** The pipeline id this node was generated from */
    pipelineId: string;
    /** The configured transformation id within a pipeline this node was generated from */
    configuredTransformationId: string;
    /** The time this node was created at */
    createdAt?: Date;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a job that executes a playground pipeline.
 */
interface PlaygroundJobRecord {
    /** Unique identifier */
    id?: string;
    jobName: JobNames;
    status: StatusEnum;
    startedAt?: Date;
    endedAt?: Date;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The partitions for this execution. */
    partitions: Record<string, string>;
    /** The IDs for the LoadedFiles this execution ran against. */
    loadedFileIds?: string[];
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Schema for the search params for an retrieval execution that can be preset for a pipeline.
 */
interface PresetRetrievalParams {
    /** Number of nodes for dense retrieval. */
    denseSimilarityTopK?: number;
    /** Number of nodes for sparse retrieval. */
    sparseSimilarityTopK?: number;
    /** Enable reranking for retrieval */
    enableReranking?: boolean;
    /** Number of reranked nodes for returning. */
    rerankTopN?: number;
    /** Alpha value for hybrid retrieval to determine the weights between dense and sparse retrieval. 0 is sparse retrieval and 1 is dense retrieval. */
    alpha?: number;
    /** Search filters for retrieval. the format of search_filters is a dict of {key: (operator, value)} */
    searchFilters?: Record<string, unknown[]>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a project.
 */
interface Project {
    name: string;
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    pipelines: Pipeline[];
    adHocEvalDatasetId?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Schema for creating a project.
 */
interface ProjectCreate {
    name: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Qdrant Vector Store.
 *
 * In this vector store, embeddings and docs are stored within a
 * Qdrant collection.
 *
 * During query time, the index uses Qdrant to query for the top
 * k most similar nodes.
 *
 * Args:
 * collection_name: (str): name of the Qdrant collection
 * client (Optional[Any]): QdrantClient instance from `qdrant-client` package
 * aclient (Optional[Any]): AsyncQdrantClient instance from `qdrant-client` package
 * url (Optional[str]): url of the Qdrant instance
 * api_key (Optional[str]): API key for authenticating with Qdrant
 * batch_size (int): number of points to upload in a single request to Qdrant. Defaults to 64
 * parallel (int): number of parallel processes to use during upload. Defaults to 1
 * max_retries (int): maximum number of retries in case of a failure. Defaults to 3
 * client_kwargs (Optional[dict]): additional kwargs for QdrantClient and AsyncQdrantClient
 * enable_hybrid (bool): whether to enable hybrid search using dense and sparse vectors
 * sparse_doc_fn (Optional[SparseEncoderCallable]): function to encode sparse vectors
 * sparse_query_fn (Optional[SparseEncoderCallable]): function to encode sparse queries
 * hybrid_fusion_fn (Optional[HybridFusionCallable]): function to fuse hybrid search results
 */
interface QdrantVectorStore {
    storesText?: boolean;
    isEmbeddingQuery?: boolean;
    flatMetadata?: boolean;
    collectionName: string;
    path?: string;
    url?: string;
    apiKey?: string;
    batchSize: number;
    parallel: number;
    maxRetries: number;
    clientKwargs?: Record<string, unknown>;
    enableHybrid: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Represents a reader and it's input arguments.
 */
interface ReaderConfig {
    /** Reader to use. */
    reader: BasePydanticReader;
    /** Reader args. */
    readerArgs?: unknown[];
    /** Reader kwargs. */
    readerKwargs?: Record<string, unknown>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Base component object to capture class names.
 */
interface RelatedNodeInfo {
    nodeId: string;
    nodeType?: ObjectType;
    metadata?: Record<string, unknown>;
    hash?: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for the result of an retrieval execution.
 */
interface RetrieveResults {
    /** The ID of the pipeline that the query was retrieved against. */
    pipelineId: string;
    /** The nodes retrieved by the pipeline for the given query. */
    retrievalNodes: TextNodeWithScore[];
    /** The end-to-end latency for retrieval and reranking. */
    retrievalLatency: Record<string, number>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * RSS reader.
 *
 * Reads content from an RSS feed.
 */
interface RssReader {
    isRemote?: boolean;
    htmlToText?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Parse text with a preference for complete sentences.
 *
 * In general, this class tries to keep sentences and paragraphs together. Therefore
 * compared to the original TokenTextSplitter, there are less likely to be
 * hanging sentences or parts of sentences at the end of the node chunk.
 */
interface SentenceSplitter {
    /** Whether or not to consider metadata when splitting. */
    includeMetadata?: boolean;
    /** Include prev/next node relationships. */
    includePrevNextRel?: boolean;
    callbackManager?: Record<string, unknown>;
    /** The token chunk size for each chunk. */
    chunkSize?: number;
    /** The token overlap of each chunk when splitting. */
    chunkOverlap?: number;
    /** Default separator for splitting into words */
    separator?: string;
    /** Separator between paragraphs. */
    paragraphSeparator?: string;
    /** Backup regex for splitting into sentences. */
    secondaryChunkingRegex?: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Simple file node parser.
 *
 * Splits a document loaded from a file into Nodes using logic based on the file type
 * automatically detects the NodeParser to use based on file type
 *
 * Args:
 * include_metadata (bool): whether to include metadata in nodes
 * include_prev_next_rel (bool): whether to include prev/next relationships
 */
interface SimpleFileNodeParser {
    /** Whether or not to consider metadata when splitting. */
    includeMetadata?: boolean;
    /** Include prev/next node relationships. */
    includePrevNextRel?: boolean;
    callbackManager?: Record<string, unknown>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Simple web page reader.
 *
 * Reads pages from the web.
 *
 * Args:
 * html_to_text (bool): Whether to convert HTML to text.
 * Requires `html2text` package.
 * metadata_fn (Optional[Callable[[str], Dict]]): A function that takes in
 * a URL and returns a dictionary of metadata.
 * Default is None.
 */
interface SimpleWebPageReader {
    isRemote?: boolean;
    htmlToText: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Slack reader.
 *
 * Reads conversations from channels. If an earliest_date is provided, an
 * optional latest_date can also be provided. If no latest_date is provided,
 * we assume the latest date is the current timestamp.
 *
 * Args:
 * slack_token (Optional[str]): Slack token. If not provided, we
 * assume the environment variable `SLACK_BOT_TOKEN` is set.
 * ssl (Optional[str]): Custom SSL context. If not provided, it is assumed
 * there is already an SSL context available.
 * earliest_date (Optional[datetime]): Earliest date from which
 * to read conversations. If not provided, we read all messages.
 * latest_date (Optional[datetime]): Latest date from which to
 * read conversations. If not provided, defaults to current timestamp
 * in combination with earliest_date.
 */
interface SlackReader {
    isRemote?: boolean;
    slackToken: string;
    earliestDateTimestamp?: number;
    latestDateTimestamp: number;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Enum for representing the status of a job
 */
type StatusEnum = "PENDING" | "SUCCESS" | "ERROR" | "CANCELED";
declare const StatusEnum: {
    readonly Pending: "PENDING";
    readonly Success: "SUCCESS";
    readonly Error: "ERROR";
    readonly Canceled: "CANCELED";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Response Schema for a supported eval LLM model.
 */
interface SupportedEvalLlmModel {
    /** The name of the supported eval LLM model. */
    name: SupportedEvalLlmModelNames;
    /** The details of the supported eval LLM model. */
    details: EvalLlmModelData;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * An enumeration.
 */
type SupportedEvalLlmModelNames = "GPT_3_5_TURBO" | "GPT_4" | "GPT_4_TURBO";
declare const SupportedEvalLlmModelNames: {
    readonly Gpt35Turbo: "GPT_3_5_TURBO";
    readonly Gpt4: "GPT_4";
    readonly Gpt4Turbo: "GPT_4_TURBO";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type TextNodeRelationshipsValue = RelatedNodeInfo | RelatedNodeInfo[];

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Base node Object.
 *
 * Generic abstract interface for retrievable nodes
 */
interface TextNode {
    /** Unique ID of the node. */
    id?: string;
    /** Embedding of the node. */
    embedding?: number[];
    /** A flat dictionary of metadata fields */
    extraInfo?: Record<string, unknown>;
    /** Metadata keys that are excluded from text for the embed model. */
    excludedEmbedMetadataKeys?: string[];
    /** Metadata keys that are excluded from text for the LLM. */
    excludedLlmMetadataKeys?: string[];
    /** A mapping of relationships to other node information. */
    relationships?: Record<string, TextNodeRelationshipsValue>;
    /** Text content of the node. */
    text?: string;
    /** Start char index of the node. */
    startCharIdx?: number;
    /** End char index of the node. */
    endCharIdx?: number;
    /** Template for how text is formatted, with {content} and {metadata_str} placeholders. */
    textTemplate?: string;
    /** Template for how metadata is formatted, with {key} and {value} placeholders. */
    metadataTemplate?: string;
    /** Separator between metadata fields when converting to string. */
    metadataSeperator?: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Same as NodeWithScore but type for node is a TextNode instead of BaseNode.
 * FastAPI doesn't accept abstract classes like BaseNode.
 */
interface TextNodeWithScore {
    node: TextNode;
    score?: number;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Implementation of splitting text that looks at word tokens.
 */
interface TokenTextSplitter {
    /** Whether or not to consider metadata when splitting. */
    includeMetadata?: boolean;
    /** Include prev/next node relationships. */
    includePrevNextRel?: boolean;
    callbackManager?: Record<string, unknown>;
    /** The token chunk size for each chunk. */
    chunkSize?: number;
    /** The token overlap of each chunk when splitting. */
    chunkOverlap?: number;
    /** Default separator for splitting into words */
    separator?: string;
    /** Additional separators for splitting. */
    backupSeparators?: unknown[];
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Trafilatura web page reader.
 *
 * Reads pages from the web.
 * Requires the `trafilatura` package.
 */
interface TrafilaturaWebReader {
    isRemote?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * An enumeration.
 */
type TransformationCategoryNames = "NODE_PARSER" | "EMBEDDING";
declare const TransformationCategoryNames: {
    readonly NodeParser: "NODE_PARSER";
    readonly Embedding: "EMBEDDING";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */
type ValidationErrorLocItem = string | number;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

interface ValidationError {
    loc: ValidationErrorLocItem[];
    msg: string;
    type: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Weaviate vector store.
 *
 * In this vector store, embeddings and docs are stored within a
 * Weaviate collection.
 *
 * During query time, the index uses Weaviate to query for the top
 * k most similar nodes.
 *
 * Args:
 * weaviate_client (weaviate.Client): WeaviateClient
 * instance from `weaviate-client` package
 * index_name (Optional[str]): name for Weaviate classes
 */
interface WeaviateVectorStore {
    storesText?: boolean;
    isEmbeddingQuery?: boolean;
    indexName: string;
    url?: string;
    textKey: string;
    authConfig?: Record<string, unknown>;
    clientKwargs?: Record<string, unknown>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Youtube Transcript reader.
 */
interface YoutubeTranscriptReader {
    isRemote?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
declare class PlatformApiError extends Error {
    readonly statusCode?: number;
    readonly body?: unknown;
    constructor({ message, statusCode, body }: {
        message?: string;
        statusCode?: number;
        body?: unknown;
    });
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

declare class UnprocessableEntityError extends PlatformApiError {
    constructor(body: HttpValidationError);
}

type index_ApiKey = ApiKey;
type index_ApiKeyCreate = ApiKeyCreate;
type index_ApiKeyUpdate = ApiKeyUpdate;
type index_AzureOpenAiEmbedding = AzureOpenAiEmbedding;
type index_BasePydanticReader = BasePydanticReader;
type index_BeautifulSoupWebReader = BeautifulSoupWebReader;
type index_BodyRunManagedRawFilesIngestionApiPipelinePipelineIdManagedIngestRawFilesPut = BodyRunManagedRawFilesIngestionApiPipelinePipelineIdManagedIngestRawFilesPut;
type index_BodyUploadFileApiParsingUploadPost = BodyUploadFileApiParsingUploadPost;
type index_BodyUpsertDataSourceFromFilesApiDataSourceFileUploadPut = BodyUpsertDataSourceFromFilesApiDataSourceFileUploadPut;
type index_ChromaVectorStore = ChromaVectorStore;
type index_CodeSplitter = CodeSplitter;
declare const index_ConfigurableDataSinkNames: typeof ConfigurableDataSinkNames;
declare const index_ConfigurableDataSourceNames: typeof ConfigurableDataSourceNames;
type index_ConfigurableTransformationDefinition = ConfigurableTransformationDefinition;
declare const index_ConfigurableTransformationNames: typeof ConfigurableTransformationNames;
type index_ConfiguredTransformationItem = ConfiguredTransformationItem;
type index_ConfiguredTransformationItemComponent = ConfiguredTransformationItemComponent;
type index_ConfiguredTransformationItemComponentOne = ConfiguredTransformationItemComponentOne;
type index_CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest = CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest;
type index_CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest = CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest;
type index_DataSink = DataSink;
type index_DataSinkComponent = DataSinkComponent;
type index_DataSinkComponentOne = DataSinkComponentOne;
type index_DataSinkCreate = DataSinkCreate;
type index_DataSinkCreateComponent = DataSinkCreateComponent;
type index_DataSinkCreateComponentOne = DataSinkCreateComponentOne;
type index_DataSinkDefinition = DataSinkDefinition;
type index_DataSinkUpdate = DataSinkUpdate;
type index_DataSinkUpdateComponent = DataSinkUpdateComponent;
type index_DataSinkUpdateComponentOne = DataSinkUpdateComponentOne;
type index_DataSource = DataSource;
type index_DataSourceComponent = DataSourceComponent;
type index_DataSourceComponentOne = DataSourceComponentOne;
type index_DataSourceCreate = DataSourceCreate;
type index_DataSourceCreateComponent = DataSourceCreateComponent;
type index_DataSourceCreateComponentOne = DataSourceCreateComponentOne;
type index_DataSourceDefinition = DataSourceDefinition;
type index_DataSourceLoadJobRecord = DataSourceLoadJobRecord;
type index_DataSourceManagedIngestionJobRecord = DataSourceManagedIngestionJobRecord;
type index_DataSourceUpdate = DataSourceUpdate;
type index_DataSourceUpdateComponent = DataSourceUpdateComponent;
type index_DataSourceUpdateComponentOne = DataSourceUpdateComponentOne;
type index_DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest = DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest;
type index_DiscordReader = DiscordReader;
type index_Document = Document;
type index_DocumentGroup = DocumentGroup;
type index_DocumentRelationshipsValue = DocumentRelationshipsValue;
type index_EvalDataset = EvalDataset;
type index_EvalDatasetCreate = EvalDatasetCreate;
type index_EvalDatasetJobRecord = EvalDatasetJobRecord;
type index_EvalDatasetUpdate = EvalDatasetUpdate;
type index_EvalExecutionCreate = EvalExecutionCreate;
type index_EvalExecutionParams = EvalExecutionParams;
type index_EvalExecutionParamsOverride = EvalExecutionParamsOverride;
type index_EvalLlmModelData = EvalLlmModelData;
type index_EvalQuestion = EvalQuestion;
type index_EvalQuestionCreate = EvalQuestionCreate;
type index_EvalQuestionResult = EvalQuestionResult;
type index_ExternallyStoredComponent = ExternallyStoredComponent;
type index_GetPipelineForProjectApiPipelinePipelineIdGetRequest = GetPipelineForProjectApiPipelinePipelineIdGetRequest;
type index_GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest = GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest;
type index_GoogleDocsReader = GoogleDocsReader;
type index_GoogleSheetsReader = GoogleSheetsReader;
type index_HtmlNodeParser = HtmlNodeParser;
type index_HttpValidationError = HttpValidationError;
declare const index_JobNames: typeof JobNames;
type index_JsonNodeParser = JsonNodeParser;
type index_ListProjectsApiProjectGetRequest = ListProjectsApiProjectGetRequest;
type index_LoadedFile = LoadedFile;
type index_LoadedFilePayload = LoadedFilePayload;
declare const index_ManagedIngestionStatus: typeof ManagedIngestionStatus;
type index_MarkdownNodeParser = MarkdownNodeParser;
type index_MetricResult = MetricResult;
type index_NotionPageReader = NotionPageReader;
declare const index_ObjectType: typeof ObjectType;
type index_OpenAiEmbedding = OpenAiEmbedding;
declare const index_ParserLanguages: typeof ParserLanguages;
type index_ParsingJob = ParsingJob;
type index_ParsingJobMarkdownResult = ParsingJobMarkdownResult;
type index_ParsingJobTextResult = ParsingJobTextResult;
type index_ParsingUsage = ParsingUsage;
type index_PgVectorStore = PgVectorStore;
type index_PineconeVectorStore = PineconeVectorStore;
type index_Pipeline = Pipeline;
type index_PipelineCreate = PipelineCreate;
type index_PipelineManagedIngestionJobRecord = PipelineManagedIngestionJobRecord;
declare const index_PipelineType: typeof PipelineType;
type index_PipelineUpdate = PipelineUpdate;
type index_PlatformTextNode = PlatformTextNode;
type index_PlatformTextNodeRelationshipsValue = PlatformTextNodeRelationshipsValue;
type index_PlaygroundJobRecord = PlaygroundJobRecord;
type index_PresetRetrievalParams = PresetRetrievalParams;
type index_Project = Project;
type index_ProjectCreate = ProjectCreate;
type index_ProjectUpdate = ProjectUpdate;
type index_QdrantVectorStore = QdrantVectorStore;
type index_ReaderConfig = ReaderConfig;
type index_RelatedNodeInfo = RelatedNodeInfo;
type index_RetrievalParams = RetrievalParams;
type index_RetrieveResults = RetrieveResults;
type index_RssReader = RssReader;
type index_SearchPipelinesApiPipelineGetRequest = SearchPipelinesApiPipelineGetRequest;
type index_SentenceSplitter = SentenceSplitter;
type index_SimpleFileNodeParser = SimpleFileNodeParser;
type index_SimpleWebPageReader = SimpleWebPageReader;
type index_SlackReader = SlackReader;
declare const index_StatusEnum: typeof StatusEnum;
type index_SupportedEvalLlmModel = SupportedEvalLlmModel;
declare const index_SupportedEvalLlmModelNames: typeof SupportedEvalLlmModelNames;
type index_TextNode = TextNode;
type index_TextNodeRelationshipsValue = TextNodeRelationshipsValue;
type index_TextNodeWithScore = TextNodeWithScore;
type index_TokenTextSplitter = TokenTextSplitter;
type index_TrafilaturaWebReader = TrafilaturaWebReader;
declare const index_TransformationCategoryNames: typeof TransformationCategoryNames;
type index_UnprocessableEntityError = UnprocessableEntityError;
declare const index_UnprocessableEntityError: typeof UnprocessableEntityError;
type index_ValidationError = ValidationError;
type index_ValidationErrorLocItem = ValidationErrorLocItem;
type index_WeaviateVectorStore = WeaviateVectorStore;
type index_YoutubeTranscriptReader = YoutubeTranscriptReader;
declare namespace index {
  export { type index_ApiKey as ApiKey, type index_ApiKeyCreate as ApiKeyCreate, type index_ApiKeyUpdate as ApiKeyUpdate, type index_AzureOpenAiEmbedding as AzureOpenAiEmbedding, type index_BasePydanticReader as BasePydanticReader, type index_BeautifulSoupWebReader as BeautifulSoupWebReader, type index_BodyRunManagedRawFilesIngestionApiPipelinePipelineIdManagedIngestRawFilesPut as BodyRunManagedRawFilesIngestionApiPipelinePipelineIdManagedIngestRawFilesPut, type index_BodyUploadFileApiParsingUploadPost as BodyUploadFileApiParsingUploadPost, type index_BodyUpsertDataSourceFromFilesApiDataSourceFileUploadPut as BodyUpsertDataSourceFromFilesApiDataSourceFileUploadPut, type index_ChromaVectorStore as ChromaVectorStore, type index_CodeSplitter as CodeSplitter, index_ConfigurableDataSinkNames as ConfigurableDataSinkNames, index_ConfigurableDataSourceNames as ConfigurableDataSourceNames, type index_ConfigurableTransformationDefinition as ConfigurableTransformationDefinition, index_ConfigurableTransformationNames as ConfigurableTransformationNames, type index_ConfiguredTransformationItem as ConfiguredTransformationItem, type index_ConfiguredTransformationItemComponent as ConfiguredTransformationItemComponent, type index_ConfiguredTransformationItemComponentOne as ConfiguredTransformationItemComponentOne, type index_CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest as CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest, type index_CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest as CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest, type index_DataSink as DataSink, type index_DataSinkComponent as DataSinkComponent, type index_DataSinkComponentOne as DataSinkComponentOne, type index_DataSinkCreate as DataSinkCreate, type index_DataSinkCreateComponent as DataSinkCreateComponent, type index_DataSinkCreateComponentOne as DataSinkCreateComponentOne, type index_DataSinkDefinition as DataSinkDefinition, type index_DataSinkUpdate as DataSinkUpdate, type index_DataSinkUpdateComponent as DataSinkUpdateComponent, type index_DataSinkUpdateComponentOne as DataSinkUpdateComponentOne, type index_DataSource as DataSource, type index_DataSourceComponent as DataSourceComponent, type index_DataSourceComponentOne as DataSourceComponentOne, type index_DataSourceCreate as DataSourceCreate, type index_DataSourceCreateComponent as DataSourceCreateComponent, type index_DataSourceCreateComponentOne as DataSourceCreateComponentOne, type index_DataSourceDefinition as DataSourceDefinition, type index_DataSourceLoadJobRecord as DataSourceLoadJobRecord, type index_DataSourceManagedIngestionJobRecord as DataSourceManagedIngestionJobRecord, type index_DataSourceUpdate as DataSourceUpdate, type index_DataSourceUpdateComponent as DataSourceUpdateComponent, type index_DataSourceUpdateComponentOne as DataSourceUpdateComponentOne, type index_DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest as DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest, type index_DiscordReader as DiscordReader, type index_Document as Document, type index_DocumentGroup as DocumentGroup, type index_DocumentRelationshipsValue as DocumentRelationshipsValue, type index_EvalDataset as EvalDataset, type index_EvalDatasetCreate as EvalDatasetCreate, type index_EvalDatasetJobRecord as EvalDatasetJobRecord, type index_EvalDatasetUpdate as EvalDatasetUpdate, type index_EvalExecutionCreate as EvalExecutionCreate, type index_EvalExecutionParams as EvalExecutionParams, type index_EvalExecutionParamsOverride as EvalExecutionParamsOverride, type index_EvalLlmModelData as EvalLlmModelData, type index_EvalQuestion as EvalQuestion, type index_EvalQuestionCreate as EvalQuestionCreate, type index_EvalQuestionResult as EvalQuestionResult, type index_ExternallyStoredComponent as ExternallyStoredComponent, type index_GetPipelineForProjectApiPipelinePipelineIdGetRequest as GetPipelineForProjectApiPipelinePipelineIdGetRequest, type index_GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest as GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest, type index_GoogleDocsReader as GoogleDocsReader, type index_GoogleSheetsReader as GoogleSheetsReader, type index_HtmlNodeParser as HtmlNodeParser, type index_HttpValidationError as HttpValidationError, index_JobNames as JobNames, type index_JsonNodeParser as JsonNodeParser, type index_ListProjectsApiProjectGetRequest as ListProjectsApiProjectGetRequest, type index_LoadedFile as LoadedFile, type index_LoadedFilePayload as LoadedFilePayload, index_ManagedIngestionStatus as ManagedIngestionStatus, type index_MarkdownNodeParser as MarkdownNodeParser, type index_MetricResult as MetricResult, type index_NotionPageReader as NotionPageReader, index_ObjectType as ObjectType, type index_OpenAiEmbedding as OpenAiEmbedding, index_ParserLanguages as ParserLanguages, type index_ParsingJob as ParsingJob, type index_ParsingJobMarkdownResult as ParsingJobMarkdownResult, type index_ParsingJobTextResult as ParsingJobTextResult, type index_ParsingUsage as ParsingUsage, type index_PgVectorStore as PgVectorStore, type index_PineconeVectorStore as PineconeVectorStore, type index_Pipeline as Pipeline, type index_PipelineCreate as PipelineCreate, type index_PipelineManagedIngestionJobRecord as PipelineManagedIngestionJobRecord, index_PipelineType as PipelineType, type index_PipelineUpdate as PipelineUpdate, type index_PlatformTextNode as PlatformTextNode, type index_PlatformTextNodeRelationshipsValue as PlatformTextNodeRelationshipsValue, type index_PlaygroundJobRecord as PlaygroundJobRecord, type index_PresetRetrievalParams as PresetRetrievalParams, type index_Project as Project, type index_ProjectCreate as ProjectCreate, type index_ProjectUpdate as ProjectUpdate, type index_QdrantVectorStore as QdrantVectorStore, type index_ReaderConfig as ReaderConfig, type index_RelatedNodeInfo as RelatedNodeInfo, type index_RetrievalParams as RetrievalParams, type index_RetrieveResults as RetrieveResults, type index_RssReader as RssReader, type index_SearchPipelinesApiPipelineGetRequest as SearchPipelinesApiPipelineGetRequest, type index_SentenceSplitter as SentenceSplitter, type index_SimpleFileNodeParser as SimpleFileNodeParser, type index_SimpleWebPageReader as SimpleWebPageReader, type index_SlackReader as SlackReader, index_StatusEnum as StatusEnum, type index_SupportedEvalLlmModel as SupportedEvalLlmModel, index_SupportedEvalLlmModelNames as SupportedEvalLlmModelNames, type index_TextNode as TextNode, type index_TextNodeRelationshipsValue as TextNodeRelationshipsValue, type index_TextNodeWithScore as TextNodeWithScore, type index_TokenTextSplitter as TokenTextSplitter, type index_TrafilaturaWebReader as TrafilaturaWebReader, index_TransformationCategoryNames as TransformationCategoryNames, index_UnprocessableEntityError as UnprocessableEntityError, type index_ValidationError as ValidationError, type index_ValidationErrorLocItem as ValidationErrorLocItem, type index_WeaviateVectorStore as WeaviateVectorStore, type index_YoutubeTranscriptReader as YoutubeTranscriptReader, index$6 as apiKey, index$1 as componentDefinition, index$9 as dataSink, index$8 as dataSource, index$3 as eval, index$7 as health, index$2 as parsing, index$4 as pipeline, index$5 as project };
}

export { index$6 as $, type ApiKey as A, type BodyUpsertDataSourceFromFilesApiDataSourceFileUploadPut as B, type CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest as C, type DataSinkCreate as D, type EvalDataset as E, type PipelineManagedIngestionJobRecord as F, type GetPipelineForProjectApiPipelinePipelineIdGetRequest as G, type RetrieveResults as H, type EvalDatasetUpdate as I, type EvalQuestion as J, type EvalQuestionCreate as K, type LoadedFile as L, type SupportedEvalLlmModel as M, type ParsingJob as N, type ParsingUsage as O, type Project as P, type ParsingJobMarkdownResult as Q, type RetrievalParams as R, type SearchPipelinesApiPipelineGetRequest as S, type ConfigurableTransformationDefinition as T, type DataSourceDefinition as U, type DataSinkDefinition as V, index as W, PlatformApiError as X, index$9 as Y, index$8 as Z, index$7 as _, type ApiKeyCreate as a, type SlackReader as a$, index$5 as a0, index$4 as a1, index$3 as a2, index$2 as a3, index$1 as a4, type DataSinkUpdateComponentOne as a5, type DataSinkUpdateComponent as a6, type DataSourceUpdateComponentOne as a7, type DataSourceUpdateComponent as a8, type BodyUploadFileApiParsingUploadPost as a9, type ExternallyStoredComponent as aA, type GoogleDocsReader as aB, type GoogleSheetsReader as aC, type HtmlNodeParser as aD, type HttpValidationError as aE, type JsonNodeParser as aF, JobNames as aG, ManagedIngestionStatus as aH, type MarkdownNodeParser as aI, type MetricResult as aJ, type NotionPageReader as aK, ObjectType as aL, type OpenAiEmbedding as aM, type PgVectorStore as aN, ParserLanguages as aO, type ParsingJobTextResult as aP, type PineconeVectorStore as aQ, PipelineType as aR, type PlatformTextNodeRelationshipsValue as aS, type PresetRetrievalParams as aT, type QdrantVectorStore as aU, type ReaderConfig as aV, type RelatedNodeInfo as aW, type RssReader as aX, type SentenceSplitter as aY, type SimpleFileNodeParser as aZ, type SimpleWebPageReader as a_, type AzureOpenAiEmbedding as aa, type BasePydanticReader as ab, type BeautifulSoupWebReader as ac, type ChromaVectorStore as ad, type CodeSplitter as ae, ConfigurableDataSinkNames as af, ConfigurableDataSourceNames as ag, ConfigurableTransformationNames as ah, type ConfiguredTransformationItemComponentOne as ai, type ConfiguredTransformationItemComponent as aj, type ConfiguredTransformationItem as ak, type DataSinkComponentOne as al, type DataSinkComponent as am, type DataSinkCreateComponentOne as an, type DataSinkCreateComponent as ao, type DataSourceComponentOne as ap, type DataSourceComponent as aq, type DataSourceCreateComponentOne as ar, type DataSourceCreateComponent as as, type DiscordReader as at, type DocumentRelationshipsValue as au, type Document as av, type DocumentGroup as aw, type EvalExecutionParams as ax, type EvalExecutionParamsOverride as ay, type EvalLlmModelData as az, type ApiKeyUpdate as b, StatusEnum as b0, SupportedEvalLlmModelNames as b1, type TextNodeRelationshipsValue as b2, type TextNode as b3, type TextNodeWithScore as b4, type TokenTextSplitter as b5, type TrafilaturaWebReader as b6, TransformationCategoryNames as b7, type ValidationErrorLocItem as b8, type ValidationError as b9, type WeaviateVectorStore as ba, type YoutubeTranscriptReader as bb, UnprocessableEntityError as bc, type DataSink as c, type DataSinkUpdate as d, type DataSourceCreate as e, type DataSource as f, type DataSourceUpdate as g, type LoadedFilePayload as h, type DataSourceLoadJobRecord as i, type ListProjectsApiProjectGetRequest as j, type ProjectCreate as k, type ProjectUpdate as l, type PipelineCreate as m, type Pipeline as n, type EvalDatasetCreate as o, type PipelineUpdate as p, type DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest as q, type PlaygroundJobRecord as r, type CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest as s, type GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest as t, type PlatformTextNode as u, type EvalDatasetJobRecord as v, type EvalExecutionCreate as w, type EvalQuestionResult as x, type BodyRunManagedRawFilesIngestionApiPipelinePipelineIdManagedIngestRawFilesPut as y, type DataSourceManagedIngestionJobRecord as z };
