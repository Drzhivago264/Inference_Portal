import { z } from "zod";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { BaseChatModel, BaseChatModelCallOptions, type BaseChatModelParams } from "@langchain/core/language_models/chat_models";
import { AIMessageChunk, BaseMessage } from "@langchain/core/messages";
import { ChatGenerationChunk, ChatResult } from "@langchain/core/outputs";
import { type OpenAICoreRequestOptions, type OpenAIClient } from "@langchain/openai";
import Groq from "groq-sdk";
import { ChatCompletionChunk } from "groq-sdk/lib/chat_completions_ext";
import { ChatCompletion, ChatCompletionCreateParams, ChatCompletionCreateParamsNonStreaming, ChatCompletionCreateParamsStreaming } from "groq-sdk/resources/chat/completions";
import { Runnable, RunnableInterface } from "@langchain/core/runnables";
import { BaseLanguageModelInput, StructuredOutputMethodOptions } from "@langchain/core/language_models/base";
import { StructuredToolInterface } from "@langchain/core/tools";
export interface ChatGroqCallOptions extends BaseChatModelCallOptions {
    headers?: Record<string, string>;
    tools?: OpenAIClient.ChatCompletionTool[];
    tool_choice?: OpenAIClient.ChatCompletionToolChoiceOption;
    response_format?: {
        type: "json_object";
    };
}
export interface ChatGroqInput extends BaseChatModelParams {
    /**
     * The Groq API key to use for requests.
     * @default process.env.GROQ_API_KEY
     */
    apiKey?: string;
    /**
     * The name of the model to use.
     * Alias for `model`
     * @default "llama2-70b-4096"
     */
    modelName?: string;
    /**
     * The name of the model to use.
     * @default "llama2-70b-4096"
     */
    model?: string;
    /**
     * Up to 4 sequences where the API will stop generating further tokens. The
     * returned text will not contain the stop sequence.
     * Alias for `stopSequences`
     */
    stop?: string | null | Array<string>;
    /**
     * Up to 4 sequences where the API will stop generating further tokens. The
     * returned text will not contain the stop sequence.
     */
    stopSequences?: Array<string>;
    /**
     * Whether or not to stream responses.
     */
    streaming?: boolean;
    /**
     * The temperature to use for sampling.
     * @default 0.7
     */
    temperature?: number;
    /**
     * The maximum number of tokens that the model can process in a single response.
     * This limits ensures computational efficiency and resource management.
     */
    maxTokens?: number;
}
type GroqRoleEnum = "system" | "assistant" | "user" | "function";
export declare function messageToGroqRole(message: BaseMessage): GroqRoleEnum;
/**
 * Wrapper around Groq API for large language models fine-tuned for chat
 *
 * Groq API is compatible to the OpenAI API with some limitations. View the
 * full API ref at:
 * @link {https://docs.api.groq.com/md/openai.oas.html}
 *
 * To use, you should have the `GROQ_API_KEY` environment variable set.
 * @example
 * ```typescript
 * const model = new ChatGroq({
 *   temperature: 0.9,
 *   apiKey: process.env.GROQ_API_KEY,
 * });
 *
 * const response = await model.invoke([new HumanMessage("Hello there!")]);
 * console.log(response);
 * ```
 */
export declare class ChatGroq extends BaseChatModel<ChatGroqCallOptions, AIMessageChunk> {
    client: Groq;
    modelName: string;
    model: string;
    temperature: number;
    stop?: string[];
    stopSequences?: string[];
    maxTokens?: number;
    streaming: boolean;
    static lc_name(): string;
    _llmType(): string;
    get lc_secrets(): {
        [key: string]: string;
    } | undefined;
    lc_serializable: boolean;
    constructor(fields?: ChatGroqInput);
    completionWithRetry(request: ChatCompletionCreateParamsStreaming, options?: OpenAICoreRequestOptions): Promise<AsyncIterable<ChatCompletionChunk>>;
    completionWithRetry(request: ChatCompletionCreateParamsNonStreaming, options?: OpenAICoreRequestOptions): Promise<ChatCompletion>;
    invocationParams(options: this["ParsedCallOptions"]): ChatCompletionCreateParams;
    bindTools(tools: (Record<string, unknown> | StructuredToolInterface)[], kwargs?: Partial<ChatGroqCallOptions>): RunnableInterface<BaseLanguageModelInput, AIMessageChunk, ChatGroqCallOptions>;
    _streamResponseChunks(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
    _generate(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    _generateNonStreaming(messages: BaseMessage[], options: this["ParsedCallOptions"], _runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    withStructuredOutput<RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: z.ZodType<RunOutput> | Record<string, any>, config?: StructuredOutputMethodOptions<false>): Runnable<BaseLanguageModelInput, RunOutput>;
    withStructuredOutput<RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: z.ZodType<RunOutput> | Record<string, any>, config?: StructuredOutputMethodOptions<true>): Runnable<BaseLanguageModelInput, {
        raw: BaseMessage;
        parsed: RunOutput;
    }>;
}
export {};
