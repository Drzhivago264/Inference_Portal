import { zodToJsonSchema } from "zod-to-json-schema";
import { BaseChatModel, } from "@langchain/core/language_models/chat_models";
import { AIMessage, AIMessageChunk, ChatMessage, ChatMessageChunk, HumanMessageChunk, SystemMessageChunk, isAIMessage, } from "@langchain/core/messages";
import { ChatGenerationChunk, } from "@langchain/core/outputs";
import { getEnvironmentVariable } from "@langchain/core/utils/env";
import { isZodSchema } from "@langchain/core/utils/types";
import Groq from "groq-sdk";
import { RunnablePassthrough, RunnableSequence, } from "@langchain/core/runnables";
import { JsonOutputParser, StructuredOutputParser, } from "@langchain/core/output_parsers";
import { JsonOutputKeyToolsParser, parseToolCall, makeInvalidToolCall, convertLangChainToolCallToOpenAI, } from "@langchain/core/output_parsers/openai_tools";
import { convertToOpenAITool } from "@langchain/core/utils/function_calling";
export function messageToGroqRole(message) {
    const type = message._getType();
    switch (type) {
        case "system":
            return "system";
        case "ai":
            return "assistant";
        case "human":
            return "user";
        case "function":
            return "function";
        case "tool":
            // Not yet supported as a type
            return "tool";
        default:
            throw new Error(`Unknown message type: ${type}`);
    }
}
function convertMessagesToGroqParams(messages) {
    return messages.map((message) => {
        if (typeof message.content !== "string") {
            throw new Error("Non string message content not supported");
        }
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        const completionParam = {
            role: messageToGroqRole(message),
            content: message.content,
            name: message.name,
            function_call: message.additional_kwargs.function_call,
            tool_calls: message.additional_kwargs.tool_calls,
            tool_call_id: message.tool_call_id,
        };
        if (isAIMessage(message) && !!message.tool_calls?.length) {
            completionParam.tool_calls = message.tool_calls.map(convertLangChainToolCallToOpenAI);
        }
        else {
            if (message.additional_kwargs.tool_calls != null) {
                completionParam.tool_calls = message.additional_kwargs.tool_calls;
            }
            if (message.tool_call_id != null) {
                completionParam.tool_call_id = message.tool_call_id;
            }
        }
        return completionParam;
    });
}
function groqResponseToChatMessage(message) {
    const rawToolCalls = message.tool_calls;
    switch (message.role) {
        case "assistant": {
            const toolCalls = [];
            const invalidToolCalls = [];
            for (const rawToolCall of rawToolCalls ?? []) {
                try {
                    toolCalls.push(parseToolCall(rawToolCall, { returnId: true }));
                    // eslint-disable-next-line @typescript-eslint/no-explicit-any
                }
                catch (e) {
                    invalidToolCalls.push(makeInvalidToolCall(rawToolCall, e.message));
                }
            }
            return new AIMessage({
                content: message.content || "",
                additional_kwargs: { tool_calls: rawToolCalls },
                tool_calls: toolCalls,
                invalid_tool_calls: invalidToolCalls,
            });
        }
        default:
            return new ChatMessage(message.content || "", message.role ?? "unknown");
    }
}
function _convertDeltaToMessageChunk(
// eslint-disable-next-line @typescript-eslint/no-explicit-any
delta) {
    const { role } = delta;
    const content = delta.content ?? "";
    let additional_kwargs;
    if (delta.function_call) {
        additional_kwargs = {
            function_call: delta.function_call,
        };
    }
    else if (delta.tool_calls) {
        additional_kwargs = {
            tool_calls: delta.tool_calls,
        };
    }
    else {
        additional_kwargs = {};
    }
    if (role === "user") {
        return new HumanMessageChunk({ content });
    }
    else if (role === "assistant") {
        return new AIMessageChunk({ content, additional_kwargs });
    }
    else if (role === "system") {
        return new SystemMessageChunk({ content });
    }
    else {
        return new ChatMessageChunk({ content, role });
    }
}
/**
 * Wrapper around Groq API for large language models fine-tuned for chat
 *
 * Groq API is compatible to the OpenAI API with some limitations. View the
 * full API ref at:
 * @link {https://docs.api.groq.com/md/openai.oas.html}
 *
 * To use, you should have the `GROQ_API_KEY` environment variable set.
 * @example
 * ```typescript
 * const model = new ChatGroq({
 *   temperature: 0.9,
 *   apiKey: process.env.GROQ_API_KEY,
 * });
 *
 * const response = await model.invoke([new HumanMessage("Hello there!")]);
 * console.log(response);
 * ```
 */
export class ChatGroq extends BaseChatModel {
    static lc_name() {
        return "ChatGroq";
    }
    _llmType() {
        return "groq";
    }
    get lc_secrets() {
        return {
            apiKey: "GROQ_API_KEY",
        };
    }
    constructor(fields) {
        super(fields ?? {});
        Object.defineProperty(this, "client", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "modelName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "llama2-70b-4096"
        });
        Object.defineProperty(this, "model", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "llama2-70b-4096"
        });
        Object.defineProperty(this, "temperature", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 0.7
        });
        Object.defineProperty(this, "stop", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "stopSequences", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "maxTokens", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "streaming", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "lc_serializable", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: true
        });
        const apiKey = fields?.apiKey || getEnvironmentVariable("GROQ_API_KEY");
        if (!apiKey) {
            throw new Error(`Groq API key not found. Please set the GROQ_API_KEY environment variable or provide the key into "apiKey"`);
        }
        this.client = new Groq({
            apiKey,
            dangerouslyAllowBrowser: true,
        });
        this.temperature = fields?.temperature ?? this.temperature;
        this.modelName = fields?.model ?? fields?.modelName ?? this.model;
        this.model = this.modelName;
        this.streaming = fields?.streaming ?? this.streaming;
        this.stop =
            fields?.stopSequences ??
                (typeof fields?.stop === "string" ? [fields.stop] : fields?.stop) ??
                [];
        this.stopSequences = this.stop;
        this.maxTokens = fields?.maxTokens;
    }
    async completionWithRetry(request, options) {
        return this.caller.call(async () => this.client.chat.completions.create(request, options));
    }
    invocationParams(options) {
        const params = super.invocationParams(options);
        if (options.tool_choice !== undefined) {
            params.tool_choice = options.tool_choice;
        }
        if (options.tools !== undefined) {
            params.tools = options.tools;
        }
        if (options.response_format !== undefined) {
            params.response_format = options.response_format;
        }
        return {
            ...params,
            stop: options.stop ?? this.stopSequences,
            model: this.model,
            temperature: this.temperature,
            max_tokens: this.maxTokens,
        };
    }
    bindTools(tools, kwargs) {
        return this.bind({
            tools: tools.map(convertToOpenAITool),
            ...kwargs,
        });
    }
    async *_streamResponseChunks(messages, options, runManager) {
        const params = this.invocationParams(options);
        const messagesMapped = convertMessagesToGroqParams(messages);
        if (options.tools !== undefined && options.tools.length > 0) {
            const result = await this._generateNonStreaming(messages, options, runManager);
            const generationMessage = result.generations[0].message;
            if (generationMessage === undefined ||
                typeof generationMessage.content !== "string") {
                throw new Error("Could not parse Groq output.");
            }
            const toolCallChunks = generationMessage.tool_calls?.map((toolCall, i) => ({
                name: toolCall.name,
                args: JSON.stringify(toolCall.args),
                id: toolCall.id,
                index: i,
            }));
            yield new ChatGenerationChunk({
                message: new AIMessageChunk({
                    content: generationMessage.content,
                    additional_kwargs: generationMessage.additional_kwargs,
                    tool_call_chunks: toolCallChunks,
                }),
                text: generationMessage.content,
            });
        }
        else {
            const response = await this.completionWithRetry({
                ...params,
                messages: messagesMapped,
                stream: true,
            }, {
                signal: options?.signal,
                headers: options?.headers,
            });
            for await (const data of response) {
                const choice = data?.choices[0];
                if (!choice) {
                    continue;
                }
                const chunk = new ChatGenerationChunk({
                    message: _convertDeltaToMessageChunk(choice.delta ?? {}),
                    text: choice.delta.content ?? "",
                    generationInfo: {
                        finishReason: choice.finish_reason,
                    },
                });
                yield chunk;
                void runManager?.handleLLMNewToken(chunk.text ?? "");
            }
            if (options.signal?.aborted) {
                throw new Error("AbortError");
            }
        }
    }
    async _generate(messages, options, runManager) {
        if (this.streaming) {
            const tokenUsage = {};
            const stream = this._streamResponseChunks(messages, options, runManager);
            const finalChunks = {};
            for await (const chunk of stream) {
                const index = chunk.generationInfo?.completion ?? 0;
                if (finalChunks[index] === undefined) {
                    finalChunks[index] = chunk;
                }
                else {
                    finalChunks[index] = finalChunks[index].concat(chunk);
                }
            }
            const generations = Object.entries(finalChunks)
                .sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10))
                .map(([_, value]) => value);
            return { generations, llmOutput: { estimatedTokenUsage: tokenUsage } };
        }
        else {
            return this._generateNonStreaming(messages, options, runManager);
        }
    }
    async _generateNonStreaming(messages, options, _runManager) {
        const tokenUsage = {};
        const params = this.invocationParams(options);
        const messagesMapped = convertMessagesToGroqParams(messages);
        const data = await this.completionWithRetry({
            ...params,
            stream: false,
            messages: messagesMapped,
        }, {
            signal: options?.signal,
            headers: options?.headers,
        });
        if ("usage" in data && data.usage) {
            const { completion_tokens: completionTokens, prompt_tokens: promptTokens, total_tokens: totalTokens, } = data.usage;
            if (completionTokens) {
                tokenUsage.completionTokens =
                    (tokenUsage.completionTokens ?? 0) + completionTokens;
            }
            if (promptTokens) {
                tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;
            }
            if (totalTokens) {
                tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;
            }
        }
        const generations = [];
        if ("choices" in data && data.choices) {
            for (const part of data.choices) {
                const text = part.message?.content ?? "";
                const generation = {
                    text,
                    message: groqResponseToChatMessage(part.message ?? { role: "assistant" }),
                };
                generation.generationInfo = {
                    ...(part.finish_reason ? { finish_reason: part.finish_reason } : {}),
                    ...(part.logprobs ? { logprobs: part.logprobs } : {}),
                };
                generations.push(generation);
            }
        }
        return {
            generations,
            llmOutput: { tokenUsage },
        };
    }
    withStructuredOutput(outputSchema, config) {
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        const schema = outputSchema;
        const name = config?.name;
        const method = config?.method;
        const includeRaw = config?.includeRaw;
        let functionName = name ?? "extract";
        let outputParser;
        let llm;
        if (method === "jsonMode") {
            llm = this.bind({
                response_format: { type: "json_object" },
            });
            if (isZodSchema(schema)) {
                outputParser = StructuredOutputParser.fromZodSchema(schema);
            }
            else {
                outputParser = new JsonOutputParser();
            }
        }
        else {
            if (isZodSchema(schema)) {
                const asJsonSchema = zodToJsonSchema(schema);
                llm = this.bind({
                    tools: [
                        {
                            type: "function",
                            function: {
                                name: functionName,
                                description: asJsonSchema.description,
                                parameters: asJsonSchema,
                            },
                        },
                    ],
                    tool_choice: {
                        type: "function",
                        function: {
                            name: functionName,
                        },
                    },
                });
                outputParser = new JsonOutputKeyToolsParser({
                    returnSingle: true,
                    keyName: functionName,
                    zodSchema: schema,
                });
            }
            else {
                let openAIFunctionDefinition;
                if (typeof schema.name === "string" &&
                    typeof schema.parameters === "object" &&
                    schema.parameters != null) {
                    openAIFunctionDefinition = schema;
                    functionName = schema.name;
                }
                else {
                    functionName = schema.title ?? functionName;
                    openAIFunctionDefinition = {
                        name: functionName,
                        description: schema.description ?? "",
                        parameters: schema,
                    };
                }
                llm = this.bind({
                    tools: [
                        {
                            type: "function",
                            function: openAIFunctionDefinition,
                        },
                    ],
                    tool_choice: {
                        type: "function",
                        function: {
                            name: functionName,
                        },
                    },
                });
                outputParser = new JsonOutputKeyToolsParser({
                    returnSingle: true,
                    keyName: functionName,
                });
            }
        }
        if (!includeRaw) {
            return llm.pipe(outputParser).withConfig({
                runName: "ChatGroqStructuredOutput",
            });
        }
        const parserAssign = RunnablePassthrough.assign({
            // eslint-disable-next-line @typescript-eslint/no-explicit-any
            parsed: (input, config) => outputParser.invoke(input.raw, config),
        });
        const parserNone = RunnablePassthrough.assign({
            parsed: () => null,
        });
        const parsedWithFallback = parserAssign.withFallbacks({
            fallbacks: [parserNone],
        });
        return RunnableSequence.from([
            {
                raw: llm,
            },
            parsedWithFallback,
        ]).withConfig({
            runName: "ChatGroqStructuredOutput",
        });
    }
}
