/**
 * This file was auto-generated by Fern from our API Definition.
 */
import * as environments from "./environments";
import * as core from "./core";
import * as Cohere from "./api";
import { EmbedJobs } from "./api/resources/embedJobs/client/Client";
import { Datasets } from "./api/resources/datasets/client/Client";
import { Connectors } from "./api/resources/connectors/client/Client";
export declare namespace CohereClient {
    interface Options {
        environment?: core.Supplier<environments.CohereEnvironment | string>;
        token: core.Supplier<core.BearerToken>;
        clientName?: core.Supplier<string | undefined>;
    }
    interface RequestOptions {
        timeoutInSeconds?: number;
        maxRetries?: number;
    }
}
export declare class CohereClient {
    protected readonly _options: CohereClient.Options;
    constructor(_options: CohereClient.Options);
    /**
     * The `chat` endpoint allows users to have conversations with a Large Language Model (LLM) from Cohere. Users can send messages as part of a persisted conversation using the `conversation_id` parameter, or they can pass in their own conversation history using the `chat_history` parameter.
     *
     * The endpoint features additional parameters such as [connectors](https://docs.cohere.com/docs/connectors) and `documents` that enable conversations enriched by external knowledge. We call this ["Retrieval Augmented Generation"](https://docs.cohere.com/docs/retrieval-augmented-generation-rag), or "RAG". For a full breakdown of the Chat API endpoint, document and connector modes, and streaming (with code samples), see [this guide](https://docs.cohere.com/docs/cochat-beta).
     */
    chatStream(request: Cohere.ChatStreamRequest, requestOptions?: CohereClient.RequestOptions): Promise<core.Stream<Cohere.StreamedChatResponse>>;
    /**
     * The `chat` endpoint allows users to have conversations with a Large Language Model (LLM) from Cohere. Users can send messages as part of a persisted conversation using the `conversation_id` parameter, or they can pass in their own conversation history using the `chat_history` parameter.
     *
     * The endpoint features additional parameters such as [connectors](https://docs.cohere.com/docs/connectors) and `documents` that enable conversations enriched by external knowledge. We call this ["Retrieval Augmented Generation"](https://docs.cohere.com/docs/retrieval-augmented-generation-rag), or "RAG". For a full breakdown of the Chat API endpoint, document and connector modes, and streaming (with code samples), see [this guide](https://docs.cohere.com/docs/cochat-beta).
     * @throws {@link Cohere.TooManyRequestsError}
     *
     * @example
     *     await cohere.chat({
     *         message: "Can you give me a global market overview of solar panels?",
     *         stream: false,
     *         chatHistory: [{
     *                 role: Cohere.ChatMessageRole.Chatbot,
     *                 message: "Hi!"
     *             }, {
     *                 role: Cohere.ChatMessageRole.Chatbot,
     *                 message: "How can I help you today?"
     *             }, {
     *                 role: Cohere.ChatMessageRole.Chatbot,
     *                 message: "message"
     *             }],
     *         promptTruncation: Cohere.ChatRequestPromptTruncation.Off,
     *         temperature: 0.3
     *     })
     */
    chat(request: Cohere.ChatRequest, requestOptions?: CohereClient.RequestOptions): Promise<Cohere.NonStreamedChatResponse>;
    /**
     * This endpoint generates realistic text conditioned on a given input.
     */
    generateStream(request: Cohere.GenerateStreamRequest, requestOptions?: CohereClient.RequestOptions): Promise<core.Stream<Cohere.GenerateStreamedResponse>>;
    /**
     * This endpoint generates realistic text conditioned on a given input.
     * @throws {@link Cohere.BadRequestError}
     * @throws {@link Cohere.TooManyRequestsError}
     * @throws {@link Cohere.InternalServerError}
     *
     * @example
     *     await cohere.generate({
     *         prompt: "Please explain to me how LLMs work",
     *         stream: false,
     *         preset: "my-preset-a58sbd"
     *     })
     */
    generate(request: Cohere.GenerateRequest, requestOptions?: CohereClient.RequestOptions): Promise<Cohere.Generation>;
    /**
     * This endpoint returns text embeddings. An embedding is a list of floating point numbers that captures semantic information about the text that it represents.
     *
     * Embeddings can be used to create text classifiers as well as empower semantic search. To learn more about embeddings, see the embedding page.
     *
     * If you want to learn more how to use the embedding model, have a look at the [Semantic Search Guide](/docs/semantic-search).
     * @throws {@link Cohere.BadRequestError}
     * @throws {@link Cohere.TooManyRequestsError}
     * @throws {@link Cohere.InternalServerError}
     */
    embed(request: Cohere.EmbedRequest, requestOptions?: CohereClient.RequestOptions): Promise<Cohere.EmbedResponse>;
    /**
     * This endpoint takes in a query and a list of texts and produces an ordered array with each text assigned a relevance score.
     * @throws {@link Cohere.TooManyRequestsError}
     *
     * @example
     *     await cohere.rerank({
     *         model: "rerank-english-v2.0",
     *         query: "What is the capital of the United States?",
     *         documents: []
     *     })
     */
    rerank(request: Cohere.RerankRequest, requestOptions?: CohereClient.RequestOptions): Promise<Cohere.RerankResponse>;
    /**
     * This endpoint makes a prediction about which label fits the specified text inputs best. To make a prediction, Classify uses the provided `examples` of text + label pairs as a reference.
     * Note: [Fine-tuned models](https://docs.cohere.com/docs/classify-fine-tuning) trained on classification examples don't require the `examples` parameter to be passed in explicitly.
     * @throws {@link Cohere.BadRequestError}
     * @throws {@link Cohere.TooManyRequestsError}
     * @throws {@link Cohere.InternalServerError}
     *
     * @example
     *     await cohere.classify({
     *         inputs: ["Confirm your email address", "hey i need u to send some $", "inputs"],
     *         examples: [{
     *                 text: "Dermatologists don't like her!",
     *                 label: "Spam"
     *             }, {
     *                 text: "Hello, open to this?",
     *                 label: "Spam"
     *             }, {
     *                 text: "I need help please wire me $1000 right now",
     *                 label: "Spam"
     *             }, {
     *                 text: "Nice to know you ;)",
     *                 label: "Spam"
     *             }, {
     *                 text: "Please help me?",
     *                 label: "Spam"
     *             }, {
     *                 text: "Your parcel will be delivered today",
     *                 label: "Not spam"
     *             }, {
     *                 text: "Review changes to our Terms and Conditions",
     *                 label: "Not spam"
     *             }, {
     *                 text: "Weekly sync notes",
     *                 label: "Not spam"
     *             }, {
     *                 text: "Re: Follow up from today\u2019s meeting",
     *                 label: "Not spam"
     *             }, {
     *                 text: "Pre-read for tomorrow",
     *                 label: "Not spam"
     *             }, {}],
     *         preset: "my-preset-a58sbd"
     *     })
     */
    classify(request: Cohere.ClassifyRequest, requestOptions?: CohereClient.RequestOptions): Promise<Cohere.ClassifyResponse>;
    /**
     * This endpoint generates a summary in English for a given text.
     * @throws {@link Cohere.TooManyRequestsError}
     *
     * @example
     *     await cohere.summarize({
     *         text: "Ice cream is a sweetened frozen food typically eaten as a snack or dessert. It may be made from milk or cream and is flavoured with a sweetener, either sugar or an alternative, and a spice, such as cocoa or vanilla, or with fruit such as strawberries or peaches. It can also be made by whisking a flavored cream base and liquid nitrogen together. Food coloring is sometimes added, in addition to stabilizers. The mixture is cooled below the freezing point of water and stirred to incorporate air spaces and to prevent detectable ice crystals from forming. The result is a smooth, semi-solid foam that is solid at very low temperatures (below 2 \u00B0C or 35 \u00B0F). It becomes more malleable as its temperature increases.\n\nThe meaning of the name \"ice cream\" varies from one country to another. In some countries, such as the United States, \"ice cream\" applies only to a specific variety, and most governments regulate the commercial use of the various terms according to the relative quantities of the main ingredients, notably the amount of cream. Products that do not meet the criteria to be called ice cream are sometimes labelled \"frozen dairy dessert\" instead. In other countries, such as Italy and Argentina, one word is used fo\r all variants. Analogues made from dairy alternatives, such as goat's or sheep's milk, or milk substitutes (e.g., soy, cashew, coconut, almond milk or tofu), are available for those who are lactose intolerant, allergic to dairy protein or vegan."
     *     })
     */
    summarize(request: Cohere.SummarizeRequest, requestOptions?: CohereClient.RequestOptions): Promise<Cohere.SummarizeResponse>;
    /**
     * This endpoint splits input text into smaller units called tokens using byte-pair encoding (BPE). To learn more about tokenization and byte pair encoding, see the tokens page.
     * @throws {@link Cohere.BadRequestError}
     * @throws {@link Cohere.TooManyRequestsError}
     * @throws {@link Cohere.InternalServerError}
     *
     * @example
     *     await cohere.tokenize({
     *         text: "tokenize me! :D",
     *         model: "command"
     *     })
     */
    tokenize(request: Cohere.TokenizeRequest, requestOptions?: CohereClient.RequestOptions): Promise<Cohere.TokenizeResponse>;
    /**
     * This endpoint takes tokens using byte-pair encoding and returns their text representation. To learn more about tokenization and byte pair encoding, see the tokens page.
     * @throws {@link Cohere.TooManyRequestsError}
     *
     * @example
     *     await cohere.detokenize({
     *         tokens: [10104, 12221, 1315, 34, 1420, 69, 1]
     *     })
     */
    detokenize(request: Cohere.DetokenizeRequest, requestOptions?: CohereClient.RequestOptions): Promise<Cohere.DetokenizeResponse>;
    protected _embedJobs: EmbedJobs | undefined;
    get embedJobs(): EmbedJobs;
    protected _datasets: Datasets | undefined;
    get datasets(): Datasets;
    protected _connectors: Connectors | undefined;
    get connectors(): Connectors;
    protected _getAuthorizationHeader(): Promise<string>;
}
