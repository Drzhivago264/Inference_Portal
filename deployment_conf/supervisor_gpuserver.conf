[program:vllm]
command = python3 -m vllm.entrypoints.api_server --model {your_model_name} --port 80 --dtype half --max-model-len {your_max_seq_length} --gpu-memory-utilization {default is 0.9 but if you get OOM drop it to 0.75}
user = root
autosart = true
autorestart = true
stdout_logfile= /home/vllm.log
